1
00:00:30,000 --> 00:00:52,000
Thank you.

2
00:01:00,000 --> 00:01:26,000
Hello, everyone.

3
00:01:26,000 --> 00:01:46,000
Hello, everyone.

4
00:01:46,000 --> 00:02:06,000
Hello, everyone.

5
00:02:06,000 --> 00:02:26,000
Hello, everyone.

6
00:02:26,000 --> 00:02:46,000
Hello, everyone.

7
00:02:46,000 --> 00:03:03,000
Hello, everyone.

8
00:03:03,000 --> 00:03:08,000
Alright, so it is 515.

9
00:03:08,000 --> 00:03:12,000
So I guess I'll just get started then.

10
00:03:12,000 --> 00:03:22,000
I'll wait for people to come in but I'll kind of give a quick introduction for them. So, for those of you who don't know me which I'm assuming is everyone.

11
00:03:22,000 --> 00:03:26,000
I, my name is Liam, and I did this class.

12
00:03:26,000 --> 00:03:30,000
Last year, and also took this class in 2018.

13
00:03:30,000 --> 00:03:40,000
So, yeah, it's the new experience for me to give this lecture to a room full of unfamiliar faces because last time I gave it, I knew all of the students because I was 13.

14
00:03:40,000 --> 00:03:43,000
We'll see how this goes.

15
00:03:43,000 --> 00:03:57,000
Um, yeah, I guess I'll just jump right in. So, first to start out, who here has taken a machine learning course at Penn so I would say, sis 525 19 count.

16
00:03:57,000 --> 00:03:58,000
Okay.

17
00:03:58,000 --> 00:04:00,000
Solid amount of hands.

18
00:04:01,000 --> 00:04:15,000
Who's taken a class that has featured machine learning in some capacity. So say like 530 or 521 or courses like that. Okay, few more hands but that still does mean that about half of the class didn't raise their hand for either who's done a project

19
00:04:15,000 --> 00:04:18,000
that involves machine learning in any capacity at all.

20
00:04:18,000 --> 00:04:24,000
Like, either personal otherwise senior design, okay even more hands. Okay, so

21
00:04:24,000 --> 00:04:29,000
a majority of you have at least some tangential experience with machine learning.

22
00:04:29,000 --> 00:04:38,000
So, today's lecture might be a bit review, but I'm hoping to give, you know, I'm hoping to have this lecture have a little bit of something for everyone.

23
00:04:38,000 --> 00:04:41,000
So,

24
00:04:41,000 --> 00:04:46,000
first to start off, I already kind of introduced myself please excuse the incredibly old headshot.

25
00:04:46,000 --> 00:04:53,000
So I'm a second year PhD student in the computer science department here. I also went here for undergrad. So, conv and robo.

26
00:04:53,000 --> 00:04:58,000
Anyone, computer engineering or robotics.

27
00:04:58,000 --> 00:05:04,000
And so my current research focuses on large language models. So if you've heard of GPT three.

28
00:05:04,000 --> 00:05:14,000
That's really what we work with nowadays and it'll be. And then also last summer I did a lot of work in speech processing and translation over at Roblox, working closely with Morgan McGuire who is a good friend of the class.

29
00:05:14,000 --> 00:05:17,000
And yeah, cool.

30
00:05:17,000 --> 00:05:26,000
So, what are we going to cover today. So today I'm going to talk about the sort of basic introduction to what a neural network is what it does, how it works.

31
00:05:26,000 --> 00:05:43,000
I'm going to talk about gradient descent. And then I'm going to give an overview of a lot of different like machine learning jargon and terminology, then go into general improvements that we've made to neural networks architecture, and then applications to tasks that you might find interesting in GPU programming.

32
00:05:43,000 --> 00:05:54,000
I think it's really important for me to start out this lecture and talk about the goals here, because I'm not trying to like teach a whole machine learning course in like one lecture.

33
00:05:54,000 --> 00:06:03,000
What I'm trying to do is to give you enough familiarity with machine learning, so that you're not as intimidated to attempt it for a final project.

34
00:06:03,000 --> 00:06:16,000
So last year we kind of piloted this whole giving a couple of machine learning lectures to this class in the hopes that we would do some final projects and it turns out that we had two really successful teams doing machine learning projects in CUDA in WebGPU.

35
00:06:16,000 --> 00:06:25,000
I don't know if Shazon has mentioned those two projects, but I will definitely be plugging them in this talk, partially because I thought they were really great and also partially because I advised them.

36
00:06:25,000 --> 00:06:32,000
But yeah, so after this lecture, you should be fairly familiar with some of the machine learning jargon.

37
00:06:32,000 --> 00:06:41,000
And I want to give you a basic understanding of how to improve model performance, what things that you can tweak to get better performance on machine learning models.

38
00:06:41,000 --> 00:06:46,000
And then hopefully by the end, you'll have a good idea of how you would apply machine learning to common tasks.

39
00:06:46,000 --> 00:06:55,000
Worth noting, though, that this lecture will be very fast because the background experience of the people in this lecture vary very widely.

40
00:06:55,000 --> 00:07:01,000
And I kind of want to have this lecture be educational for even the people that have taken a machine learning class before.

41
00:07:01,000 --> 00:07:08,000
So if you've really never looked at any machine learning whatsoever, the first half of this lecture might be really fast for you.

42
00:07:08,000 --> 00:07:20,000
Feel free to ask as many questions as you need. Also, I will link some resources or at least send resources to the teams that can link for the people that really want to follow along better.

43
00:07:20,000 --> 00:07:27,000
OK. Cool. So what is a neural network?

44
00:07:27,000 --> 00:07:37,000
So when we start thinking about machine learning, it's important to think about kind of what tasks machine learning is solving, and I think that that better contextualizes it.

45
00:07:37,000 --> 00:07:46,000
If any people have seen the 3Blue1Brown series on machine learning, this first half of the talk steals very heavily from that really good video series.

46
00:07:46,000 --> 00:07:52,000
I highly recommend watching it instead of listening very closely to my retelling of his video series.

47
00:07:52,000 --> 00:07:58,000
I actually told the students the last time I gave this lecture to watch that first before coming to class so I can just review it quickly.

48
00:07:58,000 --> 00:08:03,000
But yeah, so I'm going to basically retread a lot of the things that he talks about in his videos.

49
00:08:03,000 --> 00:08:09,000
In any case, so we want to accomplish the task, given a handwritten digit, classify what number it is.

50
00:08:09,000 --> 00:08:18,000
The way that we're going to represent the input is by a vector of length 784, where each index in the vector corresponds to a pixel value.

51
00:08:18,000 --> 00:08:22,000
So black pixels are coded as 0 and white pixels are coded as 1.

52
00:08:22,000 --> 00:08:32,000
And, you know, up here the image is shown as a matrix, but you can basically take that and extend it to be a giant column vector, and that's what we're going to think of it as.

53
00:08:32,000 --> 00:08:36,000
Okay, so here's a neural network that does this task.

54
00:08:36,000 --> 00:08:41,000
Treating this as a black box, what we're doing is we're taking an image, we're feeding it into the neural network,

55
00:08:41,000 --> 00:08:47,000
and we expect to get out a vector of size k, where k is the number of classes that we have to choose from.

56
00:08:47,000 --> 00:08:57,000
And we would assume that the class, the correct class, in this case 9, is the one that has the highest activation on the output neuron.

57
00:08:57,000 --> 00:09:06,000
So for people who have machine learning experience, what are all these lines and circles? Anyone?

58
00:09:06,000 --> 00:09:13,000
Like in this diagram.

59
00:09:13,000 --> 00:09:21,000
Yeah, exactly. Right, so the circles are the nodes, some people call them neurons, if they want to really, like, belabor the brain metaphor.

60
00:09:21,000 --> 00:09:25,000
And then the connections between the nodes are the weights.

61
00:09:25,000 --> 00:09:32,000
To be more specific, actually really quickly, I really like this visualization.

62
00:09:32,000 --> 00:09:41,000
Pass in an image, propagates the network, the weights connect all of the nodes, they light up different nodes, and then we get the output as the proper class.

63
00:09:41,000 --> 00:09:45,000
Anyway, okay, so what are these connections?

64
00:09:45,000 --> 00:09:48,000
The connections are weights.

65
00:09:49,000 --> 00:09:56,000
What a layer does is it takes a weighted sum of every single input,

66
00:09:56,000 --> 00:10:00,000
and then the output neuron is, like I said, the weighted sum of all of the inputs.

67
00:10:00,000 --> 00:10:05,000
So down here, weight 1 times input 1 plus weight 2 times input 2, so on and so forth.

68
00:10:05,000 --> 00:10:11,000
And each input is connected to every output, and every output is connected to every input, thus the fully connected.

69
00:10:11,000 --> 00:10:14,000
So this is, I'm going to be describing a fully connected layer.

70
00:10:14,000 --> 00:10:17,000
So we do this for every single output neuron.

71
00:10:17,000 --> 00:10:25,000
And so if I had, let's say, k output neurons and n input neurons, how many different weights do I have here?

72
00:10:32,000 --> 00:10:33,000
Correct.

73
00:10:37,000 --> 00:10:38,000
What was that?

74
00:10:41,000 --> 00:10:42,000
k to the n?

75
00:10:45,000 --> 00:10:46,000
Go ahead.

76
00:10:46,000 --> 00:10:47,000
n times k.

77
00:10:47,000 --> 00:10:48,000
Yeah.

78
00:10:49,000 --> 00:10:50,000
Sorry.

79
00:10:51,000 --> 00:10:53,000
Yeah, so n times k total weights.

80
00:10:53,000 --> 00:10:56,000
So there would be n weights for every k output.

81
00:10:56,000 --> 00:10:57,000
We have k outputs.

82
00:10:57,000 --> 00:10:58,000
Perfect.

83
00:10:58,000 --> 00:10:59,000
Cool.

84
00:10:59,000 --> 00:11:07,000
So importantly, if we just did this weighted sum and then had the outputs sum weighted sum over and over and over again,

85
00:11:07,000 --> 00:11:11,000
you can actually just decompose that into a gigantic weighted sum.

86
00:11:11,000 --> 00:11:21,000
So we have to add a nonlinear function for each one of these layers, or else us doing multiple layers doesn't actually make any sense.

87
00:11:24,000 --> 00:11:26,000
This is the function that we're going to be using for now.

88
00:11:26,000 --> 00:11:29,000
I'll talk a bit more about the choice of activation function.

89
00:11:29,000 --> 00:11:38,000
But for now, you can think of this function as taking an input that can vary from negative infinity to positive infinity, and then kind of clamping it to 0 and 1.

90
00:11:41,000 --> 00:11:49,000
So the equation for one neuron is this weighted sum, and then you do this sigmoid function on top of the weighted sum.

91
00:11:49,000 --> 00:11:50,000
And that's all it is.

92
00:11:52,000 --> 00:11:57,000
There's actually an extra term in here that I haven't talked about so far, and this is called the bias term.

93
00:11:58,000 --> 00:12:00,000
What do people know about bias term?

94
00:12:00,000 --> 00:12:01,000
What is it doing here?

95
00:12:05,000 --> 00:12:07,000
I kind of have it already in the slide.

96
00:12:08,000 --> 00:12:20,000
So if you have a bias term, this means that the activation, the neuron, will only activate meaningfully when the sum is above a certain value, if the bias term is negative.

97
00:12:20,000 --> 00:12:25,000
And so this means that we can scale our neuron activations independently of our input parameters.

98
00:12:27,000 --> 00:12:32,000
So, okay, this is an equation for one specific neuron.

99
00:12:33,000 --> 00:12:38,000
We have a weighted sum of all the inputs for one specific neuron, because each W is individual value.

100
00:12:38,000 --> 00:12:42,000
But how would we write an equation for an entire layer at once?

101
00:12:42,000 --> 00:12:43,000
Can we even do that?

102
00:12:47,000 --> 00:12:49,000
How would you use a vector exactly?

103
00:12:49,000 --> 00:13:03,000
Yeah, it's very similar.

104
00:13:03,000 --> 00:13:05,000
So it's a good observation.

105
00:13:05,000 --> 00:13:07,000
So this is very similar to a dot product.

106
00:13:07,000 --> 00:13:14,000
We have the, if you consider the weights as a row vector and activations as a column vector, if you dot product at this, that would be this.

107
00:13:14,000 --> 00:13:16,000
And so you have the same activations.

108
00:13:17,000 --> 00:13:19,000
So these A's would be, let's say, our input, right?

109
00:13:19,000 --> 00:13:22,000
But we have a different set of weights for every single output.

110
00:13:22,000 --> 00:13:25,000
So how do we take one column vector and dot product it with multiple row vectors?

111
00:13:29,000 --> 00:13:35,000
How do you take one column vector, let's say the A's, and dot product them with multiple different sets of weights?

112
00:13:35,000 --> 00:13:38,000
Because we have a different set of weights for every single output neuron.

113
00:13:39,000 --> 00:13:40,000
Yeah, exactly.

114
00:13:40,000 --> 00:13:41,000
So you do matrix multiply.

115
00:13:42,000 --> 00:13:46,000
So we can make this into a matrix equation to give us an equation for an entire layer.

116
00:13:46,000 --> 00:14:00,000
So writing this capital W as a matrix of row vectors that contain weights, the equation for an entire output layer is sigmoid, which is element-wise sigmoid, of weight times x plus b.

117
00:14:00,000 --> 00:14:03,000
So what are the dimensions of all of these?

118
00:14:03,000 --> 00:14:05,000
What is the dimension of W, dimension of x?

119
00:14:05,000 --> 00:14:11,000
Let's say that we have n input neurons and then k output neurons, right?

120
00:14:11,000 --> 00:14:14,000
So x would be a column vector of size what?

121
00:14:15,000 --> 00:14:16,000
N.

122
00:14:16,000 --> 00:14:20,000
And then b, sorry, our W would be a matrix of size what?

123
00:14:21,000 --> 00:14:22,000
N by k.

124
00:14:22,000 --> 00:14:24,000
And then b would be a vector of?

125
00:14:28,000 --> 00:14:29,000
Yes, k.

126
00:14:29,000 --> 00:14:32,000
And then sigmoid does this on that final vector.

127
00:14:32,000 --> 00:14:36,000
So you have a 1 by k vector after doing Wx plus b and you do sigmoid on that.

128
00:14:36,000 --> 00:14:38,000
And then that's the whole layer.

129
00:14:38,000 --> 00:14:39,000
Cool.

130
00:14:40,000 --> 00:14:41,000
All right.

131
00:14:41,000 --> 00:14:47,000
So what parameters of this equation are learned by the network or we expect them to be learned by the network?

132
00:14:50,000 --> 00:14:51,000
Exactly, right.

133
00:14:51,000 --> 00:14:55,000
And so the total number of learned parameters is n times k, which we were talking about.

134
00:14:55,000 --> 00:14:58,000
That's the amount of weights, and then plus another k for the bias terms.

135
00:14:58,000 --> 00:14:59,000
Cool.

136
00:15:03,000 --> 00:15:05,000
What is the function?

137
00:15:05,000 --> 00:15:06,000
Yeah, sure.

138
00:15:06,000 --> 00:15:07,000
Let me go back.

139
00:15:08,000 --> 00:15:12,000
What this function is, it's 1 over 1 plus e to the minus x.

140
00:15:12,000 --> 00:15:15,000
So that's just being applied to every single element in the vector.

141
00:15:15,000 --> 00:15:21,000
And the reason why we're applying it is because we have to have some nonlinear function after each layer.

142
00:15:21,000 --> 00:15:29,000
Because if we didn't, then if you had two layers in a row, so like in linear algebra,

143
00:15:29,000 --> 00:15:32,000
if you apply a matrix, and then you apply another matrix, and you apply another matrix,

144
00:15:32,000 --> 00:15:34,000
all of that can be written as just one matrix.

145
00:15:34,000 --> 00:15:39,000
So we don't get any more modeling power if we continue to just apply linear layers over and over again.

146
00:15:39,000 --> 00:15:42,000
Because what we're essentially doing is applying one linear layer.

147
00:15:42,000 --> 00:15:47,000
And so if we put nonlinearities in between, then you can actually get more power out of it.

148
00:15:47,000 --> 00:15:50,000
Is this sigmoid the activation function?

149
00:15:50,000 --> 00:15:51,000
Correct.

150
00:15:51,000 --> 00:15:52,000
Like, it could also be linear?

151
00:15:52,000 --> 00:15:53,000
Exactly.

152
00:15:53,000 --> 00:15:55,000
Yeah, I'll talk about it later on.

153
00:15:55,000 --> 00:15:56,000
Yeah, absolutely.

154
00:15:57,000 --> 00:15:58,000
Okay.

155
00:15:59,000 --> 00:16:02,000
Any other questions about linear layers?

156
00:16:03,000 --> 00:16:04,000
Especially about this equation.

157
00:16:04,000 --> 00:16:08,000
This equation will be featured in like tons of slides from this point forward.

158
00:16:08,000 --> 00:16:11,000
So, okay, cool.

159
00:16:14,000 --> 00:16:16,000
We have this nice animation.

160
00:16:17,000 --> 00:16:22,000
Okay, so a neural network, zooming back out again at a high level,

161
00:16:22,000 --> 00:16:27,000
is a function that is parameterized by a ton of weights and biases.

162
00:16:27,000 --> 00:16:30,000
So in this case, 13,000 weights and biases for a two-layer network.

163
00:16:30,000 --> 00:16:32,000
And it takes in a ton of different inputs.

164
00:16:32,000 --> 00:16:35,000
So it's a function of 784 variables.

165
00:16:35,000 --> 00:16:38,000
And then you apply all of the parameters, and then you get some output vector

166
00:16:38,000 --> 00:16:40,000
that corresponds to the class that you want.

167
00:16:41,000 --> 00:16:44,000
But how do we learn the weights and biases, right?

168
00:16:44,000 --> 00:16:46,000
I've kind of skipped over the most important part.

169
00:16:47,000 --> 00:16:49,000
Okay, so let's talk about gradient descent.

170
00:16:49,000 --> 00:16:53,000
So imagine hypothetically that we had some function, c of x,

171
00:16:53,000 --> 00:16:55,000
that told us for a specific input to the neural network

172
00:16:55,000 --> 00:16:58,000
how bad the neural network is doing at that input.

173
00:16:58,000 --> 00:17:01,000
We'd like to find the minima of this function,

174
00:17:01,000 --> 00:17:04,000
because then we can make our neural network minimally bad.

175
00:17:05,000 --> 00:17:09,000
If we found the minima, we know the values of the weights and biases

176
00:17:09,000 --> 00:17:13,000
that makes the network minimally bad for a specific input x.

177
00:17:13,000 --> 00:17:14,000
Cool.

178
00:17:16,000 --> 00:17:20,000
One possible such function is mean squared error.

179
00:17:20,000 --> 00:17:24,000
So if we have the output of our neural network,

180
00:17:24,000 --> 00:17:27,000
and we have the true output, so let's say for this image,

181
00:17:27,000 --> 00:17:31,000
the true output would be a fully activated 3D neuron and nothing else.

182
00:17:32,000 --> 00:17:36,000
We take the difference of these two outputs and then square them.

183
00:17:36,000 --> 00:17:39,000
We don't want sign difference, because then you get into the realm

184
00:17:39,000 --> 00:17:42,000
of negative and positive, and that would cancel out.

185
00:17:43,000 --> 00:17:45,000
And so we would want squared difference.

186
00:17:45,000 --> 00:17:46,000
So that's one possible such function.

187
00:17:46,000 --> 00:17:48,000
There are a bunch of different functions we could choose,

188
00:17:48,000 --> 00:17:50,000
but this seems like a reasonable choice,

189
00:17:50,000 --> 00:17:52,000
because it's just how far away are you.

190
00:17:58,000 --> 00:18:03,000
You could, but absolute value turns out to be harder to learn,

191
00:18:03,000 --> 00:18:05,000
because the distance between two vectors,

192
00:18:05,000 --> 00:18:08,000
if you're using squared distance, it's like Euclidean distance,

193
00:18:08,000 --> 00:18:10,000
whereas absolute value, it's like Manhattan distance.

194
00:18:10,000 --> 00:18:12,000
But yeah, no, that's a good point.

195
00:18:13,000 --> 00:18:14,000
And people actually do use that.

196
00:18:14,000 --> 00:18:17,000
It's a root mean squared error, and that's also a common function,

197
00:18:17,000 --> 00:18:18,000
which is square root of...

198
00:18:23,000 --> 00:18:24,000
Any other questions?

199
00:18:26,000 --> 00:18:28,000
This function choice might seem arbitrary,

200
00:18:28,000 --> 00:18:30,000
but I'll talk about it a bit more in the future.

201
00:18:32,000 --> 00:18:36,000
So what would be, again, just to make sure that everyone's on the same page,

202
00:18:36,000 --> 00:18:42,000
what would be the inputs of our function, of the cost function?

203
00:18:42,000 --> 00:18:45,000
The function that tells us how bad our neural network is for a given input.

204
00:18:45,000 --> 00:18:47,000
We have the input, right, x.

205
00:18:47,000 --> 00:18:48,000
But what else?

206
00:18:52,000 --> 00:18:54,000
Yeah, the correct answer, exactly.

207
00:19:00,000 --> 00:19:01,000
Anything else?

208
00:19:04,000 --> 00:19:06,000
What's the function parameterized by?

209
00:19:13,000 --> 00:19:14,000
Yeah, the weights and the biases.

210
00:19:14,000 --> 00:19:17,000
So the total, writing the function out properly would be like

211
00:19:17,000 --> 00:19:21,000
cost function of C, sorry, C of x, y,

212
00:19:21,000 --> 00:19:23,000
which would be the correct answer, let's say,

213
00:19:23,000 --> 00:19:25,000
and then weights and biases.

214
00:19:27,000 --> 00:19:29,000
So this is a really unwieldy function.

215
00:19:29,000 --> 00:19:32,000
It's gigantic, and the number of parameters means that this function

216
00:19:32,000 --> 00:19:35,000
is like 13,000 input variables, basically.

217
00:19:36,000 --> 00:19:38,000
And we're trying to find the minimum value of this function,

218
00:19:38,000 --> 00:19:41,000
which is a very difficult task.

219
00:19:41,000 --> 00:19:43,000
So I've kind of already preempted myself,

220
00:19:43,000 --> 00:19:46,000
but why can't we just sort of set the derivative of this function

221
00:19:46,000 --> 00:19:49,000
equal to zero and solve for the minimum?

222
00:19:53,000 --> 00:19:55,000
It seems like a totally reasonable thing to attempt, right?

223
00:19:58,000 --> 00:20:01,000
You know, we want to find all of the weights and biases

224
00:20:01,000 --> 00:20:02,000
that make the function a minimum,

225
00:20:02,000 --> 00:20:04,000
so we just take the partial with respect to every single one

226
00:20:04,000 --> 00:20:07,000
of the 13,000 weights and biases, set them all to zero,

227
00:20:07,000 --> 00:20:09,000
and then solve for those values.

228
00:20:10,000 --> 00:20:13,000
Well, it turns out that this function is not a linear function,

229
00:20:13,000 --> 00:20:16,000
and so unless you have a nice algorithm to solve 13,000

230
00:20:16,000 --> 00:20:20,000
simultaneous equations, this is generally not feasible.

231
00:20:20,000 --> 00:20:23,000
And even if it was feasible, you don't really want to solve

232
00:20:23,000 --> 00:20:26,000
that equation for a specific input x,

233
00:20:26,000 --> 00:20:29,000
because then your neural network would be very good at classifying one x.

234
00:20:29,000 --> 00:20:31,000
You want to solve it for the set of all x's,

235
00:20:31,000 --> 00:20:33,000
which makes this even harder,

236
00:20:33,000 --> 00:20:35,000
because you want to find some average solution.

237
00:20:36,000 --> 00:20:39,000
So you can see why people don't just actually solve

238
00:20:39,000 --> 00:20:41,000
for the closed form of this.

239
00:20:41,000 --> 00:20:43,000
So what can we do instead of solving for closed form?

240
00:20:45,000 --> 00:20:52,000
So an easier method is if we have some value for our function,

241
00:20:52,000 --> 00:20:55,000
for the weights and biases that we have,

242
00:20:55,000 --> 00:20:57,000
and we take the derivative of that function,

243
00:20:57,000 --> 00:21:01,000
and we step the weights and biases in the direction of that derivative.

244
00:21:01,000 --> 00:21:04,000
So let's say we have some step size parameter eta.

245
00:21:04,000 --> 00:21:06,000
Eventually, if we keep taking steps,

246
00:21:06,000 --> 00:21:09,000
we will converge to some minimum value of this function,

247
00:21:09,000 --> 00:21:11,000
and this is called gradient descent.

248
00:21:11,000 --> 00:21:15,000
Obviously, in this diagram, it's only two dimensions,

249
00:21:15,000 --> 00:21:16,000
so there's no gradient.

250
00:21:16,000 --> 00:21:18,000
It's just the derivative, but obviously,

251
00:21:18,000 --> 00:21:20,000
our function is not two-dimensional.

252
00:21:20,000 --> 00:21:22,000
It's just for ease of visualization.

253
00:21:23,000 --> 00:21:25,000
So I have a couple of questions on this slide.

254
00:21:25,000 --> 00:21:31,000
So why would the step size parameter not be learned by the function?

255
00:21:31,000 --> 00:21:33,000
Why do we have to supply this?

256
00:21:33,000 --> 00:21:34,000
Yes.

257
00:21:55,000 --> 00:22:00,000
Yeah, I mean, so I would say the reason,

258
00:22:00,000 --> 00:22:03,000
the real reason why we can't learn the step size parameter

259
00:22:03,000 --> 00:22:05,000
is because very similar to what you said,

260
00:22:05,000 --> 00:22:07,000
like if we tried to learn the step size parameter,

261
00:22:07,000 --> 00:22:11,000
how would we go about learning with gradient descent?

262
00:22:11,000 --> 00:22:13,000
But the step size parameter is part of gradient descent, right?

263
00:22:13,000 --> 00:22:16,000
You can't learn the parameters of gradient descent with gradient descent, right?

264
00:22:16,000 --> 00:22:20,000
But yeah, and eventually, we're converged to the minimum,

265
00:22:20,000 --> 00:22:25,000
but is that, okay, this is a very obvious answer to this question, right?

266
00:22:25,000 --> 00:22:27,000
This function can have a ton of minima.

267
00:22:27,000 --> 00:22:29,000
There's no guarantee at all that it's convex.

268
00:22:29,000 --> 00:22:31,000
It's definitely not.

269
00:22:31,000 --> 00:22:34,000
And so, yeah, there's no guarantee that there's a global minimum.

270
00:22:36,000 --> 00:22:38,000
This is obviously very important for machine learning.

271
00:22:38,000 --> 00:22:42,000
It means that training the same network with the same data

272
00:22:42,000 --> 00:22:44,000
multiple times with different initializations

273
00:22:44,000 --> 00:22:46,000
means that you're going to get different accuracies.

274
00:22:46,000 --> 00:22:48,000
It's just how it works.

275
00:22:48,000 --> 00:22:49,000
Okay.

276
00:22:51,000 --> 00:22:52,000
So it's a nice visualization.

277
00:22:52,000 --> 00:22:56,000
You can imagine these balls being initialized.

278
00:22:56,000 --> 00:22:57,000
Sorry, hold on.

279
00:22:58,000 --> 00:23:01,000
Starting on the curve would be some point

280
00:23:01,000 --> 00:23:03,000
where you have a certain amount of weights.

281
00:23:03,000 --> 00:23:06,000
And we start taking gradient descent,

282
00:23:06,000 --> 00:23:09,000
and all of these balls sort of step in the direction of the gradient

283
00:23:09,000 --> 00:23:11,000
and roll down to the local minimum.

284
00:23:13,000 --> 00:23:17,000
How much of ML is just like tuning the guess at C?

285
00:23:19,000 --> 00:23:21,000
We like not to think about that.

286
00:23:21,000 --> 00:23:25,000
No, there's, I mean, it's obviously,

287
00:23:25,000 --> 00:23:27,000
you're not supposed to tune your random seeds

288
00:23:27,000 --> 00:23:29,000
or tune your initialization.

289
00:23:29,000 --> 00:23:31,000
You're supposed to develop techniques that,

290
00:23:31,000 --> 00:23:33,000
regardless of the initialization or seed,

291
00:23:33,000 --> 00:23:36,000
on average get you better performance than other people.

292
00:23:36,000 --> 00:23:39,000
But you can definitely cheat it a bit

293
00:23:39,000 --> 00:23:42,000
where if you get a very specifically very good random seed.

294
00:23:44,000 --> 00:23:45,000
Exactly, yeah.

295
00:23:45,000 --> 00:23:48,000
So that's to prevent, especially when you have,

296
00:23:48,000 --> 00:23:50,000
let's say if you're submitting to some leaderboard

297
00:23:50,000 --> 00:23:52,000
for a very specific task,

298
00:23:52,000 --> 00:23:55,000
like they'll hold the test data themselves

299
00:23:55,000 --> 00:23:58,000
to make sure that you can't specifically tune

300
00:23:58,000 --> 00:24:00,000
the initializations and the parameters

301
00:24:00,000 --> 00:24:02,000
so that you get really high accuracy by cheating.

302
00:24:02,000 --> 00:24:04,000
I mean, if you, hypothetically,

303
00:24:04,000 --> 00:24:06,000
you had population size data,

304
00:24:06,000 --> 00:24:11,000
tuning the random seed would give you the optimal results.

305
00:24:11,000 --> 00:24:13,000
It would, but it's really unsatisfying.

306
00:24:13,000 --> 00:24:17,000
And I think the only scenario where you would do that

307
00:24:17,000 --> 00:24:19,000
is if you're in a machine learning class

308
00:24:19,000 --> 00:24:21,000
and you're trying to get extra credit

309
00:24:21,000 --> 00:24:23,000
from a position on a leaderboard.

310
00:24:23,000 --> 00:24:25,000
Other than that, yeah.

311
00:24:25,000 --> 00:24:27,000
But that is true.

312
00:24:28,000 --> 00:24:29,000
Okay.

313
00:24:29,000 --> 00:24:31,000
Any other questions?

314
00:24:31,000 --> 00:24:33,000
Okay, cool.

315
00:24:34,000 --> 00:24:36,000
So moving to the multivariable case,

316
00:24:36,000 --> 00:24:38,000
we've talked about the 2D function.

317
00:24:38,000 --> 00:24:40,000
The derivative for multivariable functions

318
00:24:40,000 --> 00:24:41,000
is called the gradient.

319
00:24:41,000 --> 00:24:44,000
And the gradient is the direction of the steepest increase.

320
00:24:44,000 --> 00:24:47,000
So the negative gradient would be the direction

321
00:24:47,000 --> 00:24:49,000
of the steepest decrease.

322
00:24:49,000 --> 00:24:52,000
And so to update our weights,

323
00:24:52,000 --> 00:24:56,000
we take the weight matrix at time step n plus one

324
00:24:56,000 --> 00:24:58,000
as equal to the weight matrix at time step n

325
00:24:58,000 --> 00:25:00,000
minus our step size parameter

326
00:25:00,000 --> 00:25:04,000
times this upside down delta is the symbol for a gradient.

327
00:25:04,000 --> 00:25:06,000
And then C is our cost function.

328
00:25:06,000 --> 00:25:08,000
And we have our input.

329
00:25:08,000 --> 00:25:12,000
The correct answer why is implicit here for some reason.

330
00:25:12,000 --> 00:25:15,000
But yes, we have input and then the weight matrix.

331
00:25:16,000 --> 00:25:20,000
Note here, I'm actually not writing the bias term explicitly

332
00:25:20,000 --> 00:25:22,000
because technically speaking,

333
00:25:22,000 --> 00:25:24,000
you can consider the bias term

334
00:25:24,000 --> 00:25:26,000
as just another column in the matrix

335
00:25:26,000 --> 00:25:28,000
if you just add another column to your input vector

336
00:25:28,000 --> 00:25:30,000
and give that value zero.

337
00:25:30,000 --> 00:25:33,000
So sometimes I will swap between

338
00:25:33,000 --> 00:25:35,000
considering everything as one big weight matrix

339
00:25:35,000 --> 00:25:38,000
and separating out the weight and the bias terms.

340
00:25:38,000 --> 00:25:41,000
I'll let you guys know when I'm swapping between those.

341
00:25:41,000 --> 00:25:44,000
But yes, this is just to make the equation pretty.

342
00:25:44,000 --> 00:25:46,000
Okay, cool.

343
00:25:48,000 --> 00:25:50,000
And this is another very nice visualization.

344
00:25:50,000 --> 00:25:54,000
So the gradient as a vector is going to have one value

345
00:25:54,000 --> 00:25:57,000
for each weight slash bias in the entire network.

346
00:25:57,000 --> 00:25:59,000
And the value of the gradient is going to tell you

347
00:25:59,000 --> 00:26:04,000
what small changes to that weight will do to the output.

348
00:26:04,000 --> 00:26:07,000
So if we have a value of a gradient that's like 3.2,

349
00:26:07,000 --> 00:26:09,000
that means that small changes to that weight

350
00:26:09,000 --> 00:26:13,000
will have a large effect on the output loss or cost.

351
00:26:13,000 --> 00:26:15,000
But a gradient value of 0.1 means that

352
00:26:15,000 --> 00:26:16,000
small changes to that weight

353
00:26:16,000 --> 00:26:18,000
really won't affect the output much at all.

354
00:26:18,000 --> 00:26:20,000
And so you can imagine that this is going to give you

355
00:26:20,000 --> 00:26:22,000
like basically importance values

356
00:26:22,000 --> 00:26:24,000
for all of your weights for a specific input.

357
00:26:24,000 --> 00:26:28,000
Like if I went to classify the image for three

358
00:26:28,000 --> 00:26:30,000
and I had a really high gradient on one of these,

359
00:26:30,000 --> 00:26:32,000
that means that changing that will really change

360
00:26:32,000 --> 00:26:34,000
how correct I was

361
00:26:34,000 --> 00:26:37,000
for predicting that specific input, image three.

362
00:26:39,000 --> 00:26:41,000
Okay, cool.

363
00:26:42,000 --> 00:26:44,000
So a few conceptual questions.

364
00:26:44,000 --> 00:26:46,000
So I've already asked this input question.

365
00:26:46,000 --> 00:26:48,000
I don't know why I have this again on here.

366
00:26:48,000 --> 00:26:50,000
I forgot, I guess I asked it earlier.

367
00:26:50,000 --> 00:26:53,000
But does the cost function change based on the input x

368
00:26:53,000 --> 00:26:55,000
and should it?

369
00:27:08,000 --> 00:27:09,000
Definitely, right?

370
00:27:09,000 --> 00:27:11,000
So our cost function changes completely

371
00:27:11,000 --> 00:27:13,000
based on the given input image.

372
00:27:13,000 --> 00:27:16,000
But if we're trying to learn a network

373
00:27:16,000 --> 00:27:18,000
that can classify any image,

374
00:27:19,000 --> 00:27:21,000
yeah, we would love it if the cost function

375
00:27:21,000 --> 00:27:23,000
was just a general cost function

376
00:27:23,000 --> 00:27:25,000
that applied to the set of all possible images,

377
00:27:25,000 --> 00:27:28,000
like the distribution of all possible handwritten digits.

378
00:27:28,000 --> 00:27:30,000
So we did not have to deal with any specific image.

379
00:27:30,000 --> 00:27:32,000
Obviously, that's infeasible to actually calculate,

380
00:27:32,000 --> 00:27:35,000
but we'll get into ways that you can approximate

381
00:27:35,000 --> 00:27:37,000
values of that cost function

382
00:27:37,000 --> 00:27:40,000
and then optimize that one instead of any one particular one.

383
00:27:42,000 --> 00:27:44,000
Another thing, when we calculate the gradient

384
00:27:44,000 --> 00:27:46,000
with respect to the weights and biases,

385
00:27:46,000 --> 00:27:48,000
are we actually calculating the full gradient

386
00:27:48,000 --> 00:27:51,000
of this function or not?

387
00:27:51,000 --> 00:27:53,000
I see people shaking their heads.

388
00:27:56,000 --> 00:27:59,000
What would the full gradient of the function look like?

389
00:28:08,000 --> 00:28:09,000
Yeah.

390
00:28:20,000 --> 00:28:22,000
So if I had a function that depended on three variables,

391
00:28:22,000 --> 00:28:24,000
x, y, z, the gradient would involve the partial

392
00:28:24,000 --> 00:28:26,000
with respect to each of those three variables.

393
00:28:26,000 --> 00:28:29,000
So that would be the full gradient of the function.

394
00:28:30,000 --> 00:28:33,000
But this function depends on the variables

395
00:28:33,000 --> 00:28:36,000
that we throw in, so the weights, the input.

396
00:28:37,000 --> 00:28:40,000
But when we're calculating the gradient for gradient descent,

397
00:28:40,000 --> 00:28:42,000
what are we not calculating?

398
00:28:43,000 --> 00:28:45,000
We calculate the partial with respect to all of the weights

399
00:28:45,000 --> 00:28:47,000
and all of the biases,

400
00:28:47,000 --> 00:28:50,000
but we're not calculating partial with respect to what?

401
00:28:59,000 --> 00:29:00,000
Yeah.

402
00:29:01,000 --> 00:29:04,000
So why would it be nonsensical to calculate the gradient

403
00:29:04,000 --> 00:29:07,000
with respect to the input points and step in that direction?

404
00:29:11,000 --> 00:29:12,000
Anyone else?

405
00:29:14,000 --> 00:29:16,000
Yeah, exactly. You can't change the input image.

406
00:29:17,000 --> 00:29:21,000
It'd be nice if our image was not the way it is, but it is.

407
00:29:22,000 --> 00:29:24,000
The gradient with respect to the image would tell you

408
00:29:24,000 --> 00:29:26,000
how you could change the image such that your network

409
00:29:26,000 --> 00:29:27,000
did better at it.

410
00:29:27,000 --> 00:29:29,000
That's not the direction we want to go.

411
00:29:29,000 --> 00:29:31,000
We want to change our network such that it's better

412
00:29:31,000 --> 00:29:32,000
at the image.

413
00:29:33,000 --> 00:29:34,000
Cool.

414
00:29:35,000 --> 00:29:38,000
I skipped the last question because I think we talked about it a bit.

415
00:29:38,000 --> 00:29:41,000
All right, so let's actually go ahead and calculate the gradient.

416
00:29:41,000 --> 00:29:42,000
Let's manually do it.

417
00:29:43,000 --> 00:29:47,000
As you can see, we have our gradient upwards delta c,

418
00:29:47,000 --> 00:29:51,000
and then each of the values in the gradient

419
00:29:51,000 --> 00:29:53,000
is the partial of c, which is the cost function,

420
00:29:53,000 --> 00:29:55,000
with respect to a parameter.

421
00:29:55,000 --> 00:30:00,000
So this is d weight 1 would be the weight of the first layer,

422
00:30:00,000 --> 00:30:03,000
and then db1 would be the bias of the first layer,

423
00:30:03,000 --> 00:30:05,000
and so on and so forth for all of the layers.

424
00:30:07,000 --> 00:30:10,000
People call this backpropagation because you get some sort of value

425
00:30:10,000 --> 00:30:13,000
at the end of your neural network, which is the cost,

426
00:30:13,000 --> 00:30:15,000
and then you calculate the derivatives by going backwards

427
00:30:15,000 --> 00:30:16,000
from the cost.

428
00:30:17,000 --> 00:30:20,000
It's really just calculus. It's just the derivative chain rule.

429
00:30:21,000 --> 00:30:23,000
I'm going to go through it for two layers.

430
00:30:24,000 --> 00:30:25,000
Bear with me for this.

431
00:30:26,000 --> 00:30:31,000
Okay, so here are the three equations that are involved

432
00:30:31,000 --> 00:30:34,000
in going through the very last layer of a network.

433
00:30:34,000 --> 00:30:37,000
This one, which is the weight times a,

434
00:30:37,000 --> 00:30:40,000
aka the activation of the previous layer, note l minus 1,

435
00:30:41,000 --> 00:30:43,000
and then the bias term.

436
00:30:43,000 --> 00:30:45,000
So we have weights times activation plus bias.

437
00:30:45,000 --> 00:30:47,000
The derivative of this with respect to the weights

438
00:30:47,000 --> 00:30:50,000
is just the activation of the previous layer.

439
00:30:50,000 --> 00:30:53,000
Everybody's okay with that? Yeah.

440
00:30:55,000 --> 00:30:59,000
This term is activations equals the output of this equation

441
00:30:59,000 --> 00:31:01,000
put into the sigmoid function.

442
00:31:02,000 --> 00:31:05,000
So the actual activation of your neuron is not just the output

443
00:31:05,000 --> 00:31:08,000
of weights times activation plus bias, but it's actually

444
00:31:08,000 --> 00:31:11,000
weight activation plus bias put into sigmoid,

445
00:31:11,000 --> 00:31:13,000
and the derivative of this is just the derivative of sigmoid,

446
00:31:13,000 --> 00:31:15,000
which it turns out is very easy to do.

447
00:31:15,000 --> 00:31:18,000
And then finally, we had the mean squared error function.

448
00:31:18,000 --> 00:31:21,000
So our cost is the activation minus the true value y squared,

449
00:31:21,000 --> 00:31:24,000
and the derivative of this is just 2 times a minus y.

450
00:31:25,000 --> 00:31:28,000
And so computing the derivative with respect to the weights

451
00:31:28,000 --> 00:31:31,000
of a specific layer is you just multiply.

452
00:31:31,000 --> 00:31:34,000
So you have the activation times derivative of sigmoid

453
00:31:34,000 --> 00:31:36,000
times 2 activation minus y.

454
00:31:36,000 --> 00:31:38,000
Done. Easy.

455
00:31:39,000 --> 00:31:42,000
And you can do this over and over and over again

456
00:31:42,000 --> 00:31:44,000
and just keep going backwards.

457
00:31:44,000 --> 00:31:46,000
So go ahead.

458
00:31:46,000 --> 00:31:48,000
Sure, sure. Go for it.

459
00:31:48,000 --> 00:31:55,000
Why is the derivative of ZL just the activation?

460
00:31:56,000 --> 00:32:00,000
Why is the derivative with respect to ZL just the activations?

461
00:32:00,000 --> 00:32:02,000
Sorry, can you say that again?

462
00:32:02,000 --> 00:32:10,000
Basically, could you just explain a bit further why the first line?

463
00:32:10,000 --> 00:32:12,000
Oh, this first line. Yeah, yeah, yeah. Okay, sure.

464
00:32:12,000 --> 00:32:15,000
So what we're doing is we're taking derivative with respect

465
00:32:15,000 --> 00:32:18,000
to this weight, right?

466
00:32:18,000 --> 00:32:22,000
And so since we have activation times weight,

467
00:32:22,000 --> 00:32:26,000
the derivative of like, you know, if it makes it easier,

468
00:32:26,000 --> 00:32:28,000
we can think of weight as just like x.

469
00:32:28,000 --> 00:32:32,000
And if you have a times x, the derivative of a times x is a.

470
00:32:37,000 --> 00:32:39,000
Everyone clear?

471
00:32:39,000 --> 00:32:41,000
On Wednesday, I'm going to do this with matrices and vectors.

472
00:32:41,000 --> 00:32:43,000
So just get ready.

473
00:32:43,000 --> 00:32:45,000
This is with one input.

474
00:32:45,000 --> 00:32:48,000
So I'm leaving all the other stuff out.

475
00:32:50,000 --> 00:32:52,000
Okay.

476
00:32:52,000 --> 00:32:54,000
Cool.

477
00:32:54,000 --> 00:32:56,000
And so like I was saying earlier,

478
00:32:56,000 --> 00:32:58,000
you can just keep going backwards with this.

479
00:32:58,000 --> 00:33:01,000
And so the chain rule, you can just do over and over again.

480
00:33:01,000 --> 00:33:05,000
So the derivative with respect to the weights on the L minus one layer

481
00:33:05,000 --> 00:33:08,000
is just, you know, all of these terms cancel out, right?

482
00:33:08,000 --> 00:33:10,000
We have derivative with respect to activations,

483
00:33:10,000 --> 00:33:12,000
which is then inputs,

484
00:33:12,000 --> 00:33:14,000
which is then activations of the previous layer,

485
00:33:14,000 --> 00:33:16,000
which is then inputs to the previous layer,

486
00:33:16,000 --> 00:33:18,000
which then are determined by the weights.

487
00:33:21,000 --> 00:33:23,000
However, this is actually wrong.

488
00:33:23,000 --> 00:33:27,000
So this equation is not correct at all.

489
00:33:28,000 --> 00:33:31,000
Can anyone tell me why this equation is actually wrong?

490
00:33:32,000 --> 00:33:34,000
It's kind of subtle though.

491
00:33:38,000 --> 00:33:40,000
There's like a slight complication with this.

492
00:34:02,000 --> 00:34:03,000
Okay.

493
00:34:15,000 --> 00:34:18,000
So not quite, but.

494
00:34:19,000 --> 00:34:23,000
We don't need to take into consideration the weight of the first layer

495
00:34:23,000 --> 00:34:28,000
because we basically will get there by the time we calculate the derivative

496
00:34:28,000 --> 00:34:30,000
with respect to that layer, right?

497
00:34:30,000 --> 00:34:33,000
Like basically you can consider the inputs to the L minus one,

498
00:34:33,000 --> 00:34:36,000
L minus one layer as constant,

499
00:34:36,000 --> 00:34:39,000
and then just sort of use this to calculate how much weight

500
00:34:39,000 --> 00:34:42,000
you're going to get by the time you calculate the derivative with respect to

501
00:34:42,000 --> 00:34:44,000
that layer, right?

502
00:34:44,000 --> 00:34:45,000
As constant,

503
00:34:45,000 --> 00:34:50,000
and then just sort of use this to calculate how much changes to that affect

504
00:34:50,000 --> 00:34:55,000
the output and changes to that do not depend on the previous layers.

505
00:34:55,000 --> 00:34:57,000
So no, it's not that either.

506
00:34:59,000 --> 00:35:01,000
So the, the,

507
00:35:01,000 --> 00:35:07,000
the subtlety here is that since every single neuron is connected to every

508
00:35:07,000 --> 00:35:09,000
single output neuron,

509
00:35:09,000 --> 00:35:13,000
that means that in order to calculate how one neuron affects the neurons

510
00:35:13,000 --> 00:35:15,000
further down in the network,

511
00:35:15,000 --> 00:35:20,000
you actually have to take the derivative with respect to all of the different

512
00:35:20,000 --> 00:35:21,000
neurons in the next layer,

513
00:35:21,000 --> 00:35:27,000
and then sum them all up to get the value for how one neuron affects the next

514
00:35:27,000 --> 00:35:29,000
layers, right?

515
00:35:29,000 --> 00:35:32,000
So just to quickly chuck this up,

516
00:35:32,000 --> 00:35:38,000
the actual derivative is the sum across all of the different activations.

517
00:35:38,000 --> 00:35:41,000
So an easier way rather than looking at this equation,

518
00:35:41,000 --> 00:35:43,000
because this is very hard to parse,

519
00:35:43,000 --> 00:35:50,000
easier way to think about it is for each weight that contributes some amount to

520
00:35:50,000 --> 00:35:52,000
a given neuron.

521
00:35:52,000 --> 00:35:57,000
So we have to take the partial with respect to each of the weights.

522
00:35:57,000 --> 00:36:00,000
And for every neuron, there are K output weights.

523
00:36:00,000 --> 00:36:03,000
And so you do have to sum up across all of the different output neurons,

524
00:36:03,000 --> 00:36:05,000
but this is the only complication.

525
00:36:05,000 --> 00:36:08,000
And then you can just keep doing this over and over and over again and back

526
00:36:08,000 --> 00:36:09,000
propagating.

527
00:36:09,000 --> 00:36:11,000
Cool.

528
00:36:11,000 --> 00:36:12,000
All right.

529
00:36:12,000 --> 00:36:15,000
So this is the most densely difficult thing to think about in this lecture.

530
00:36:15,000 --> 00:36:17,000
The rest is going to be kind of smooth sailing,

531
00:36:17,000 --> 00:36:22,000
but I think it's important to just sort of demystify that this is like basic

532
00:36:22,000 --> 00:36:23,000
calculus.

533
00:36:23,000 --> 00:36:28,000
And we can do this in more detail on Wednesday and slash implementing it in

534
00:36:28,000 --> 00:36:30,000
more detail and watch how it's implemented in CUDA,

535
00:36:30,000 --> 00:36:33,000
because as I'm sure you guys can tell, this is embarrassingly parallel.

536
00:36:33,000 --> 00:36:37,000
We can compute all of these in parallel, sum them all up and yeah.

537
00:36:37,000 --> 00:36:39,000
Cool.

538
00:36:39,000 --> 00:36:40,000
All right.

539
00:36:40,000 --> 00:36:46,000
So traditional gradient descent takes one step for every pass through the

540
00:36:46,000 --> 00:36:48,000
entirety of the training data.

541
00:36:48,000 --> 00:36:50,000
The reason for that is, again,

542
00:36:50,000 --> 00:36:53,000
we don't want to optimize the function for one specific input.

543
00:36:53,000 --> 00:36:56,000
We want to optimize the function for a given set of training data.

544
00:36:56,000 --> 00:37:00,000
So what we do is we get the loss function or we get the, sorry,

545
00:37:00,000 --> 00:37:05,000
we get the gradient for every single item in our training data set.

546
00:37:05,000 --> 00:37:07,000
And then we average all of those gradients together.

547
00:37:07,000 --> 00:37:10,000
And then we take a step in the direction of that average gradient.

548
00:37:10,000 --> 00:37:11,000
And in some senses,

549
00:37:11,000 --> 00:37:14,000
that's like taking a step in the direction of the real cost function,

550
00:37:14,000 --> 00:37:18,000
not a cost function for any one specific image.

551
00:37:18,000 --> 00:37:20,000
Does that make sense?

552
00:37:20,000 --> 00:37:23,000
There's a really important distinction.

553
00:37:35,000 --> 00:37:42,000
Yeah, absolutely, Ken.

554
00:37:42,000 --> 00:37:46,000
And I will talk about that when I get to that sort of improvements to basic

555
00:37:46,000 --> 00:37:48,000
neural networks, but that's called an annealing schedule.

556
00:37:48,000 --> 00:37:53,000
And yeah, like almost everyone does that.

557
00:37:53,000 --> 00:37:56,000
Does everyone understand the fact that gradient descent tends to take,

558
00:37:56,000 --> 00:37:59,000
or the traditional gradient descent takes one step for every pass to the

559
00:37:59,000 --> 00:38:01,000
entire training data.

560
00:38:01,000 --> 00:38:03,000
We don't want to take a step for a specific input.

561
00:38:03,000 --> 00:38:05,000
We want to take a step over the whole training data.

562
00:38:05,000 --> 00:38:09,000
The problem is that this is actually super duper slow because we have to pass

563
00:38:09,000 --> 00:38:14,000
through the entirety of our training data to take one optimization step.

564
00:38:14,000 --> 00:38:19,000
So to approximate that, we actually do take one step per input example.

565
00:38:19,000 --> 00:38:22,000
And we just hope that like in expectation,

566
00:38:22,000 --> 00:38:25,000
that's going to end up being the same as taking one step for every average

567
00:38:25,000 --> 00:38:26,000
gradient.

568
00:38:26,000 --> 00:38:29,000
So here, this very low res diagram,

569
00:38:30,000 --> 00:38:35,000
the purple is what happens when you take one step per input image.

570
00:38:35,000 --> 00:38:38,000
And then the blue is what happens when you take one step per the entirety of

571
00:38:38,000 --> 00:38:39,000
the training data.

572
00:38:39,000 --> 00:38:43,000
And this sort of like the contour line plot is some imagining of like some

573
00:38:43,000 --> 00:38:47,000
perfect cost function that doesn't depend on input.

574
00:38:47,000 --> 00:38:51,000
So the nickname for this gradient descent is called stochastic gradient

575
00:38:51,000 --> 00:38:52,000
descent.

576
00:38:52,000 --> 00:38:53,000
There's nothing stochastic about this.

577
00:38:53,000 --> 00:38:57,000
It just looks very random when you take a step per every individual input

578
00:38:57,000 --> 00:38:58,000
image.

579
00:38:58,000 --> 00:39:01,000
But yeah, it's a really, really bad thing.

580
00:39:01,000 --> 00:39:05,000
With GPUs though, we can do like mini batch stochastic gradient descent,

581
00:39:05,000 --> 00:39:10,000
where we basically just take a step for 32 images in parallel.

582
00:39:10,000 --> 00:39:14,000
So we compute the gradient for 32 different images in parallel,

583
00:39:14,000 --> 00:39:17,000
and then we can average them and take a step in that direction.

584
00:39:17,000 --> 00:39:20,000
And so that's much, much, much better.

585
00:39:20,000 --> 00:39:22,000
But still, this is how neural networks are actually trained.

586
00:39:22,000 --> 00:39:26,000
We take mini batch stochastic gradient descent.

587
00:39:26,000 --> 00:39:27,000
Okay.

588
00:39:27,000 --> 00:39:31,000
Any questions?

589
00:39:31,000 --> 00:39:34,000
Sweet.

590
00:39:34,000 --> 00:39:35,000
Okay.

591
00:39:35,000 --> 00:39:36,000
A lot of text.

592
00:39:36,000 --> 00:39:37,000
I'll just read it.

593
00:39:37,000 --> 00:39:40,000
Neural network is linear layers, WX plus B.

594
00:39:40,000 --> 00:39:44,000
The cost or loss function tells us how bad our network is doing,

595
00:39:44,000 --> 00:39:46,000
given a specific X, W, and B.

596
00:39:46,000 --> 00:39:50,000
The weights and biases of a network are learned from our data by minimizing

597
00:39:50,000 --> 00:39:51,000
through gradient descent.

598
00:39:51,000 --> 00:39:55,000
Gradient descent is a process by which we randomly initialize weights and

599
00:39:55,000 --> 00:39:56,000
biases,

600
00:39:56,000 --> 00:39:59,000
and then iteratively update them by taking a fixed size step in the

601
00:39:59,000 --> 00:40:01,000
direction of the negative gradient.

602
00:40:01,000 --> 00:40:04,000
And then we calculate the gradient through back propagation,

603
00:40:04,000 --> 00:40:06,000
which is just applying the chain rule.

604
00:40:06,000 --> 00:40:08,000
And then we use mini batch stochastic gradient descent.

605
00:40:08,000 --> 00:40:11,000
Cool.

606
00:40:11,000 --> 00:40:12,000
All right.

607
00:40:12,000 --> 00:40:16,000
So let's get to some jargon.

608
00:40:16,000 --> 00:40:19,000
So activation function, I've mentioned this already,

609
00:40:19,000 --> 00:40:22,000
but this is the nonlinear function that we apply in between layers.

610
00:40:22,000 --> 00:40:25,000
This can be any nonlinear function.

611
00:40:25,000 --> 00:40:27,000
The cost function or the loss function is what I was saying.

612
00:40:27,000 --> 00:40:30,000
It's the function that tells us how bad our neural network is doing.

613
00:40:30,000 --> 00:40:32,000
This also can be any function.

614
00:40:32,000 --> 00:40:34,000
Some function choices make more sense than others.

615
00:40:34,000 --> 00:40:39,000
The input representation is how we represent our input as a vector.

616
00:40:39,000 --> 00:40:41,000
So in the case of an image,

617
00:40:41,000 --> 00:40:44,000
we have the pixels corresponding to black or white,

618
00:40:44,000 --> 00:40:46,000
but in more complicated things like language,

619
00:40:46,000 --> 00:40:49,000
that's not always obvious how you represent your input as a vector,

620
00:40:49,000 --> 00:40:52,000
but you need to in order to pass it into a neural network.

621
00:40:52,000 --> 00:40:56,000
The parameters are the trainable numbers in a neural network,

622
00:40:56,000 --> 00:40:58,000
aka the weights and biases.

623
00:40:58,000 --> 00:41:00,000
And the architecture is how all of them are connected.

624
00:41:00,000 --> 00:41:03,000
Finally,

625
00:41:03,000 --> 00:41:09,000
optimization method is what variant of gradient descent do you use?

626
00:41:09,000 --> 00:41:11,000
So do you use normal gradient descent, stochastic gradient descent?

627
00:41:11,000 --> 00:41:13,000
Do you use with an annealing schedule?

628
00:41:13,000 --> 00:41:15,000
Do you use an adaptive learning rate? All this stuff.

629
00:41:15,000 --> 00:41:18,000
The hyper parameters are the parameters that are not learned.

630
00:41:18,000 --> 00:41:20,000
And the initialization is what you start with,

631
00:41:20,000 --> 00:41:22,000
the starting weight and bias.

632
00:41:22,000 --> 00:41:27,000
This is just jargon so that if I say one of these words later on,

633
00:41:27,000 --> 00:41:32,000
people don't get confused.

634
00:41:32,000 --> 00:41:34,000
All right.

635
00:41:34,000 --> 00:41:37,000
Finally,

636
00:41:37,000 --> 00:41:39,000
traditionally when we use machine learning,

637
00:41:39,000 --> 00:41:43,000
we like to split up our data into three different sets.

638
00:41:43,000 --> 00:41:47,000
So the training set is the images that we actually do gradient descent on.

639
00:41:47,000 --> 00:41:51,000
The development set is the set of data that we use to tune our hyper parameter.

640
00:41:51,000 --> 00:41:53,000
So in the case of learning rate,

641
00:41:53,000 --> 00:41:55,000
which learning rate gives us the best accuracy,

642
00:41:55,000 --> 00:41:59,000
we can look at the accuracy on the development set of data.

643
00:41:59,000 --> 00:42:02,000
And that's how we can decide what the best learning rate is.

644
00:42:02,000 --> 00:42:06,000
And this usually involves just manually retraining the entire network over and

645
00:42:06,000 --> 00:42:09,000
over and over and over again with different values of the hyper parameters.

646
00:42:09,000 --> 00:42:10,000
Because again,

647
00:42:10,000 --> 00:42:13,000
there's no way to learn a parameter that's involved in gradient descent.

648
00:42:13,000 --> 00:42:16,000
And then finally we have the test data.

649
00:42:16,000 --> 00:42:19,000
And it's important to split up development and test data because you don't want to

650
00:42:19,000 --> 00:42:23,000
actually over optimize to a specific set of test data because you're kind of

651
00:42:23,000 --> 00:42:26,000
cheating. Similar to the question from earlier.

652
00:42:26,000 --> 00:42:30,000
Like if you tune your initialization variables and you're learning rate to a

653
00:42:30,000 --> 00:42:31,000
specific set of data,

654
00:42:31,000 --> 00:42:34,000
you're not really going to be able to generalize to new sets of data.

655
00:42:34,000 --> 00:42:38,000
Or I should say the accuracy on that set of data isn't representative of the

656
00:42:38,000 --> 00:42:42,000
accuracy on other sets of data. And then finally overfitting,

657
00:42:43,000 --> 00:42:46,000
which is if you fully train a neural network to full convergence,

658
00:42:46,000 --> 00:42:49,000
like you just do gradient descent over and over and over again until there's

659
00:42:49,000 --> 00:42:51,000
never any more updates. They're all zero.

660
00:42:51,000 --> 00:42:55,000
The network is powerful enough that it will just memorize the training data and

661
00:42:55,000 --> 00:42:57,000
it loses the ability to generalize.

662
00:42:57,000 --> 00:43:01,000
This is a very common phenomenon when you have a model that has way more

663
00:43:01,000 --> 00:43:06,000
parameters than your function really necessitates and neural networks tend to.

664
00:43:06,000 --> 00:43:09,000
The way to fix this is just stop early.

665
00:43:09,000 --> 00:43:12,000
Stop when you have good accuracy on your validation set.

666
00:43:18,000 --> 00:43:21,000
All of the things that I've talked about is the state of machine learning in

667
00:43:21,000 --> 00:43:23,000
the 1990s.

668
00:43:25,000 --> 00:43:27,000
We can make a lot of improvements to these.

669
00:43:27,000 --> 00:43:30,000
Let's take neural networks into the 21st century.

670
00:43:32,000 --> 00:43:34,000
The task that I've been talking about so far,

671
00:43:34,000 --> 00:43:38,000
the recognizing handwritten digits is a famous task in machine learning and

672
00:43:38,000 --> 00:43:42,000
the data set of handwritten digits is called MNIST.

673
00:43:42,000 --> 00:43:45,000
Don't ask what the acronym stands for, I forget.

674
00:43:45,000 --> 00:43:49,000
And it was released in 1998 and a three layer neural network with all of the

675
00:43:49,000 --> 00:43:52,000
things that we just talked about. So stochastic gradient descent,

676
00:43:52,000 --> 00:43:55,000
learning rate, all this stuff, got about 97% accuracy.

677
00:43:55,000 --> 00:43:58,000
And I think someone can correct me if I'm wrong,

678
00:43:58,000 --> 00:44:02,000
but this is one of the first occasions where a neural network got really,

679
00:44:02,000 --> 00:44:05,000
really good accuracy and it was shown that it would be practical to actually

680
00:44:05,000 --> 00:44:07,000
use. Nowadays,

681
00:44:07,000 --> 00:44:12,000
the accuracy of the best networks on MNIST are so

682
00:44:12,000 --> 00:44:15,000
good that it's almost meaningless to compare between each other.

683
00:44:15,000 --> 00:44:18,000
Almost every network is 99.7%.

684
00:44:19,000 --> 00:44:23,000
But what's the difference? What got us that last 3%?

685
00:44:23,000 --> 00:44:26,000
99.7% is basically perfect,

686
00:44:26,000 --> 00:44:29,000
given that some of the images in MNIST kind of look like this.

687
00:44:30,000 --> 00:44:34,000
I don't know if you could consider that having a label of eight or that

688
00:44:34,000 --> 00:44:38,000
having a label of four. I think 99.7% is good enough.

689
00:44:38,000 --> 00:44:41,000
So what changed?

690
00:44:43,000 --> 00:44:47,000
So I would say the biggest difference between older neural networks and

691
00:44:47,000 --> 00:44:50,000
newer neural networks is depth.

692
00:44:50,000 --> 00:44:53,000
So when people say deep learning,

693
00:44:53,000 --> 00:44:57,000
all they're doing is doing machine learning, but with more layers.

694
00:44:58,000 --> 00:45:01,000
So there's nothing fundamentally different about deep learning versus

695
00:45:01,000 --> 00:45:04,000
machine learning. It's just machine learning, but bigger.

696
00:45:04,000 --> 00:45:07,000
This is very, very expensive.

697
00:45:07,000 --> 00:45:11,000
Just to remind us, the amount of learnable parameters in a given layer is

698
00:45:11,000 --> 00:45:15,000
n times k. And so you keep adding layers on,

699
00:45:15,000 --> 00:45:18,000
that's another factor of n times k. And for large n and large k,

700
00:45:18,000 --> 00:45:22,000
obviously n times k is huge. But this is made possible by GPUs.

701
00:45:22,000 --> 00:45:25,000
This is totally infeasible to train on the CPU,

702
00:45:25,000 --> 00:45:29,000
but with the advent of GPUs and with people getting better at

703
00:45:29,000 --> 00:45:32,000
parallelizing and having a lot more ability to do things in parallel,

704
00:45:32,000 --> 00:45:35,000
we've been able to train deeper and deeper networks and therefore the

705
00:45:35,000 --> 00:45:39,000
accuracy goes up. And importantly,

706
00:45:42,000 --> 00:45:45,000
while deep learning theory is still incredibly young,

707
00:45:46,000 --> 00:45:50,000
we know that depth improves performance in almost every scenario,

708
00:45:50,000 --> 00:45:53,000
and we don't really know why.

709
00:45:53,000 --> 00:45:56,000
There are theories on why.

710
00:45:56,000 --> 00:46:00,000
This one is an image from a 2019 paper that describes the double descent

711
00:46:00,000 --> 00:46:03,000
phenomenon, where after a certain amount of parameters,

712
00:46:03,000 --> 00:46:06,000
if you add more, instead of overfitting,

713
00:46:06,000 --> 00:46:10,000
those parameters seem to generalize on top of other parameters that have

714
00:46:10,000 --> 00:46:13,000
overfit. Very hand wavy.

715
00:46:13,000 --> 00:46:16,000
Someone that is more involved with actual theory of scale and machine

716
00:46:16,000 --> 00:46:19,000
learning can talk better about this.

717
00:46:20,000 --> 00:46:24,000
But the consensus is that depth works very well, but it is expensive.

718
00:46:24,000 --> 00:46:28,000
We currently do not know the limits of how well depth works.

719
00:46:28,000 --> 00:46:31,000
Like if every single task is just improvable by adding more layers.

720
00:46:36,000 --> 00:46:39,000
Okay. The next improvement, besides just scaling,

721
00:46:39,000 --> 00:46:41,000
is in the loss function.

722
00:46:41,000 --> 00:46:44,000
So earlier we were talking about mean squared error as a loss function.

723
00:46:44,000 --> 00:46:47,000
Changing the loss function, obviously,

724
00:46:47,000 --> 00:46:50,000
can drastically alter what is learned because it is literally what we back

725
00:46:50,000 --> 00:46:51,000
propagate.

726
00:46:51,000 --> 00:46:54,000
The value of the loss function determines every single update we make to all

727
00:46:54,000 --> 00:46:56,000
of the weights.

728
00:46:56,000 --> 00:46:59,000
So instead of using mean squared error or alternative root mean squared

729
00:46:59,000 --> 00:47:03,000
error, is there a better loss function for classification in particular?

730
00:47:06,000 --> 00:47:09,000
So reminder that in the case of MNIST,

731
00:47:09,000 --> 00:47:12,000
our output is a vector of size K, where K is the number of classes.

732
00:47:13,000 --> 00:47:16,000
Man.

733
00:47:16,000 --> 00:47:19,000
It is weird. The last time I gave this lecture, it also thunderstormed like

734
00:47:19,000 --> 00:47:22,000
this in the middle of the lecture.

735
00:47:22,000 --> 00:47:25,000
It is a sign.

736
00:47:25,000 --> 00:47:28,000
So each class in the output vector gets a score based on the output of the

737
00:47:28,000 --> 00:47:31,000
final layer.

738
00:47:33,000 --> 00:47:36,000
However, let's take a hypothetical.

739
00:47:36,000 --> 00:47:39,000
Let's say that our neural network predicted zero for every class and then

740
00:47:39,000 --> 00:47:42,000
for three predicted like 10.

741
00:47:42,000 --> 00:47:45,000
But the true answer is zero for every class, but at three, a value of one.

742
00:47:47,000 --> 00:47:50,000
What would our loss be for that example?

743
00:47:54,000 --> 00:47:57,000
Sure, sure.

744
00:47:57,000 --> 00:48:00,000
So let's say our neural network output a vector that instead of containing

745
00:48:00,000 --> 00:48:03,000
like in this example, we have like 0.43, 0.28, whatnot.

746
00:48:04,000 --> 00:48:07,000
Let's say that it was very confident that the image was three, so much so

747
00:48:07,000 --> 00:48:10,000
that it gave three, the index corresponding to class three, a weight of

748
00:48:10,000 --> 00:48:13,000
five, and every other class, a weight of zero.

749
00:48:13,000 --> 00:48:16,000
However, the true answer gave the weight for class three one and

750
00:48:16,000 --> 00:48:19,000
everything else zero.

751
00:48:19,000 --> 00:48:22,000
What would mean squared error give us as a loss?

752
00:48:25,000 --> 00:48:28,000
Yeah, it would give you a really big value because we predicted five

753
00:48:28,000 --> 00:48:31,000
instead of one.

754
00:48:31,000 --> 00:48:34,000
This seems wrong.

755
00:48:34,000 --> 00:48:37,000
If the network is 100% confident and gave all of its weight to the right

756
00:48:37,000 --> 00:48:40,000
class, that should be zero loss.

757
00:48:40,000 --> 00:48:43,000
It got it perfectly correct.

758
00:48:43,000 --> 00:48:46,000
So how do we incorporate this intuition into our loss function?

759
00:48:46,000 --> 00:48:49,000
What we can do, exactly.

760
00:48:49,000 --> 00:48:52,000
Normalize it, but then also normalize it in log space.

761
00:48:52,000 --> 00:48:55,000
Exactly.

762
00:48:55,000 --> 00:48:58,000
So if we put the output scores through this softmax function, which is

763
00:48:58,000 --> 00:49:01,000
normalizing it, but in log space, then we can turn any output into a

764
00:49:01,000 --> 00:49:04,000
probability distribution.

765
00:49:04,000 --> 00:49:07,000
So the highest value gets given the highest probability, and it scales

766
00:49:07,000 --> 00:49:10,000
logarithmically with respect to how large your output is with respect to

767
00:49:10,000 --> 00:49:13,000
all of the other things in the vector.

768
00:49:13,000 --> 00:49:16,000
Absolutely.

769
00:49:16,000 --> 00:49:19,000
Yep.

770
00:49:20,000 --> 00:49:23,000
Cool.

771
00:49:23,000 --> 00:49:26,000
So in our example from earlier, if we had five to the correct class, after

772
00:49:26,000 --> 00:49:29,000
we put it through softmax, that should be one, and therefore we would get

773
00:49:29,000 --> 00:49:32,000
zero mean squared error after putting it through softmax.

774
00:49:32,000 --> 00:49:35,000
Okay.

775
00:49:35,000 --> 00:49:38,000
However, now we're going to do something a little bit more complicated.

776
00:49:38,000 --> 00:49:41,000
We're going to do something a little bit more complicated.

777
00:49:41,000 --> 00:49:44,000
We're going to do something a little bit more complicated.

778
00:49:44,000 --> 00:49:47,000
We're going to do something a little bit more complicated.

779
00:49:47,000 --> 00:49:50,000
However, now we're comparing two probability distributions with each

780
00:49:50,000 --> 00:49:53,000
other.

781
00:49:53,000 --> 00:49:56,000
So our true distribution is zero and one in the correct class, and the

782
00:49:56,000 --> 00:49:59,000
actual distribution that we get from our network is the softmax

783
00:49:59,000 --> 00:50:02,000
distribution.

784
00:50:02,000 --> 00:50:05,000
When we're comparing two distributions, does it make sense to use mean

785
00:50:05,000 --> 00:50:08,000
squared error?

786
00:50:08,000 --> 00:50:11,000
Put a different way.

787
00:50:11,000 --> 00:50:14,000
Is a change in probability of 89% to 99% the same as a change in

788
00:50:15,000 --> 00:50:18,000
probability of 59% to 69%, or should it be?

789
00:50:18,000 --> 00:50:21,000
No, right?

790
00:50:21,000 --> 00:50:24,000
Like, it should be much more important to go from 89% to 99%, or much more

791
00:50:24,000 --> 00:50:27,000
difficult, I shouldn't say.

792
00:50:27,000 --> 00:50:30,000
You should get a much higher loss if you guess 89% for a 99% than if you

793
00:50:30,000 --> 00:50:33,000
guess 59% for a 69%.

794
00:50:33,000 --> 00:50:36,000
So is there a mathematical concept that compares how different two

795
00:50:36,000 --> 00:50:39,000
probability distributions are?

796
00:50:39,000 --> 00:50:42,000
.

797
00:50:42,000 --> 00:50:45,000
.

798
00:50:45,000 --> 00:50:48,000
.

799
00:50:48,000 --> 00:50:51,000
.

800
00:50:51,000 --> 00:50:54,000
.

801
00:50:54,000 --> 00:50:57,000
.

802
00:50:57,000 --> 00:51:00,000
.

803
00:51:00,000 --> 00:51:03,000
.

804
00:51:03,000 --> 00:51:06,000
.

805
00:51:06,000 --> 00:51:09,000
.

806
00:51:09,000 --> 00:51:12,000
.

807
00:51:12,000 --> 00:51:15,000
.

808
00:51:15,000 --> 00:51:18,000
.

809
00:51:18,000 --> 00:51:21,000
.

810
00:51:21,000 --> 00:51:24,000
.

811
00:51:24,000 --> 00:51:27,000
.

812
00:51:27,000 --> 00:51:30,000
.

813
00:51:30,000 --> 00:51:33,000
.

814
00:51:33,000 --> 00:51:36,000
.

815
00:51:36,000 --> 00:51:39,000
.

816
00:51:39,000 --> 00:51:42,000
.

817
00:51:42,000 --> 00:51:45,000
.

818
00:51:45,000 --> 00:51:48,000
.

819
00:51:48,000 --> 00:51:51,000
.

820
00:51:51,000 --> 00:51:54,000
.

821
00:51:54,000 --> 00:51:57,000
.

822
00:51:57,000 --> 00:52:00,000
.

823
00:52:00,000 --> 00:52:03,000
.

824
00:52:03,000 --> 00:52:06,000
.

825
00:52:06,000 --> 00:52:09,000
.

826
00:52:09,000 --> 00:52:12,000
.

827
00:52:12,000 --> 00:52:15,000
.

828
00:52:15,000 --> 00:52:18,000
.

829
00:52:18,000 --> 00:52:21,000
.

830
00:52:21,000 --> 00:52:24,000
.

831
00:52:24,000 --> 00:52:27,000
.

832
00:52:27,000 --> 00:52:30,000
.

833
00:52:30,000 --> 00:52:33,000
.

834
00:52:33,000 --> 00:52:36,000
.

835
00:52:36,000 --> 00:52:39,000
.

836
00:52:39,000 --> 00:52:42,000
.

837
00:52:42,000 --> 00:52:45,000
.

838
00:52:45,000 --> 00:52:48,000
.

839
00:52:48,000 --> 00:52:51,000
.

840
00:52:51,000 --> 00:52:54,000
.

841
00:52:54,000 --> 00:52:57,000
.

842
00:52:57,000 --> 00:53:00,000
.

843
00:53:00,000 --> 00:53:03,000
.

844
00:53:03,000 --> 00:53:06,000
.

845
00:53:06,000 --> 00:53:09,000
.

846
00:53:09,000 --> 00:53:12,000
.

847
00:53:12,000 --> 00:53:15,000
.

848
00:53:15,000 --> 00:53:18,000
.

849
00:53:18,000 --> 00:53:21,000
.

850
00:53:21,000 --> 00:53:24,000
.

851
00:53:24,000 --> 00:53:27,000
.

852
00:53:27,000 --> 00:53:30,000
.

853
00:53:30,000 --> 00:53:33,000
.

854
00:53:33,000 --> 00:53:36,000
.

855
00:53:36,000 --> 00:53:39,000
.

856
00:53:39,000 --> 00:53:42,000
.

857
00:53:42,000 --> 00:53:45,000
.

858
00:53:45,000 --> 00:53:48,000
.

859
00:53:48,000 --> 00:53:51,000
.

860
00:53:51,000 --> 00:53:54,000
.

861
00:53:54,000 --> 00:53:57,000
.

862
00:53:57,000 --> 00:54:00,000
.

863
00:54:00,000 --> 00:54:03,000
.

864
00:54:03,000 --> 00:54:06,000
.

865
00:54:06,000 --> 00:54:09,000
.

866
00:54:09,000 --> 00:54:12,000
.

867
00:54:12,000 --> 00:54:15,000
.

868
00:54:15,000 --> 00:54:18,000
.

869
00:54:18,000 --> 00:54:21,000
.

870
00:54:21,000 --> 00:54:24,000
.

871
00:54:24,000 --> 00:54:27,000
.

872
00:54:27,000 --> 00:54:30,000
.

873
00:54:30,000 --> 00:54:33,000
.

874
00:54:33,000 --> 00:54:36,000
.

875
00:54:36,000 --> 00:54:39,000
.

876
00:54:39,000 --> 00:54:42,000
.

877
00:54:42,000 --> 00:54:45,000
.

878
00:54:45,000 --> 00:54:48,000
.

879
00:54:48,000 --> 00:54:51,000
.

880
00:54:51,000 --> 00:54:54,000
.

881
00:54:54,000 --> 00:54:57,000
.

882
00:54:57,000 --> 00:55:00,000
.

883
00:55:00,000 --> 00:55:03,000
.

884
00:55:03,000 --> 00:55:06,000
.

885
00:55:06,000 --> 00:55:09,000
.

886
00:55:09,000 --> 00:55:12,000
.

887
00:55:12,000 --> 00:55:15,000
.

888
00:55:15,000 --> 00:55:18,000
.

889
00:55:18,000 --> 00:55:21,000
.

890
00:55:21,000 --> 00:55:24,000
.

891
00:55:24,000 --> 00:55:27,000
.

892
00:55:27,000 --> 00:55:30,000
.

893
00:55:30,000 --> 00:55:33,000
.

894
00:55:33,000 --> 00:55:36,000
.

895
00:55:36,000 --> 00:55:39,000
.

896
00:55:39,000 --> 00:55:42,000
.

897
00:55:42,000 --> 00:55:45,000
.

898
00:55:45,000 --> 00:55:48,000
.

899
00:55:48,000 --> 00:55:51,000
.

900
00:55:51,000 --> 00:55:54,000
.

901
00:55:54,000 --> 00:55:57,000
.

902
00:55:57,000 --> 00:56:00,000
.

903
00:56:00,000 --> 00:56:03,000
.

904
00:56:03,000 --> 00:56:06,000
.

905
00:56:06,000 --> 00:56:09,000
.

906
00:56:09,000 --> 00:56:12,000
.

907
00:56:12,000 --> 00:56:15,000
.

908
00:56:15,000 --> 00:56:18,000
.

909
00:56:18,000 --> 00:56:21,000
.

910
00:56:21,000 --> 00:56:24,000
.

911
00:56:24,000 --> 00:56:27,000
.

912
00:56:27,000 --> 00:56:30,000
.

913
00:56:30,000 --> 00:56:33,000
.

914
00:56:33,000 --> 00:56:36,000
.

915
00:56:36,000 --> 00:56:39,000
.

916
00:56:39,000 --> 00:56:42,000
.

917
00:56:42,000 --> 00:56:45,000
.

918
00:56:45,000 --> 00:56:48,000
.

919
00:56:48,000 --> 00:56:51,000
.

920
00:56:51,000 --> 00:56:54,000
.

921
00:56:54,000 --> 00:56:57,000
.

922
00:56:57,000 --> 00:57:00,000
.

923
00:57:00,000 --> 00:57:03,000
.

924
00:57:03,000 --> 00:57:06,000
.

925
00:57:06,000 --> 00:57:09,000
.

926
00:57:09,000 --> 00:57:12,000
.

927
00:57:12,000 --> 00:57:15,000
.

928
00:57:15,000 --> 00:57:18,000
.

929
00:57:18,000 --> 00:57:21,000
.

930
00:57:21,000 --> 00:57:24,000
.

931
00:57:24,000 --> 00:57:27,000
.

932
00:57:27,000 --> 00:57:30,000
.

933
00:57:30,000 --> 00:57:33,000
.

934
00:57:33,000 --> 00:57:36,000
.

935
00:57:36,000 --> 00:57:39,000
.

936
00:57:39,000 --> 00:57:42,000
.

937
00:57:42,000 --> 00:57:45,000
.

938
00:57:45,000 --> 00:57:48,000
.

939
00:57:48,000 --> 00:57:51,000
.

940
00:57:51,000 --> 00:57:54,000
.

941
00:57:54,000 --> 00:57:57,000
.

942
00:57:57,000 --> 00:58:00,000
.

943
00:58:00,000 --> 00:58:03,000
.

944
00:58:03,000 --> 00:58:06,000
.

945
00:58:06,000 --> 00:58:09,000
.

946
00:58:09,000 --> 00:58:12,000
.

947
00:58:12,000 --> 00:58:15,000
.

948
00:58:15,000 --> 00:58:18,000
.

949
00:58:18,000 --> 00:58:21,000
.

950
00:58:21,000 --> 00:58:24,000
.

951
00:58:24,000 --> 00:58:27,000
.

952
00:58:27,000 --> 00:58:30,000
.

953
00:58:30,000 --> 00:58:33,000
.

954
00:58:33,000 --> 00:58:36,000
.

955
00:58:36,000 --> 00:58:39,000
.

956
00:58:39,000 --> 00:58:42,000
.

957
00:58:42,000 --> 00:58:45,000
.

958
00:58:45,000 --> 00:58:48,000
.

959
00:58:48,000 --> 00:58:51,000
.

960
00:58:51,000 --> 00:58:54,000
.

961
00:58:54,000 --> 00:58:57,000
.

962
00:58:57,000 --> 00:59:00,000
.

963
00:59:00,000 --> 00:59:03,000
.

964
00:59:03,000 --> 00:59:06,000
.

965
00:59:06,000 --> 00:59:09,000
.

966
00:59:09,000 --> 00:59:12,000
.

967
00:59:12,000 --> 00:59:15,000
.

968
00:59:15,000 --> 00:59:18,000
.

969
00:59:18,000 --> 00:59:21,000
.

970
00:59:21,000 --> 00:59:24,000
.

971
00:59:24,000 --> 00:59:27,000
.

972
00:59:27,000 --> 00:59:30,000
.

973
00:59:30,000 --> 00:59:33,000
.

974
00:59:33,000 --> 00:59:36,000
.

975
00:59:36,000 --> 00:59:39,000
.

976
00:59:39,000 --> 00:59:42,000
.

977
00:59:42,000 --> 00:59:45,000
.

978
00:59:45,000 --> 00:59:48,000
.

979
00:59:48,000 --> 00:59:51,000
.

980
00:59:51,000 --> 00:59:54,000
.

981
00:59:54,000 --> 00:59:57,000
.

982
00:59:57,000 --> 01:00:00,000
.

983
01:00:00,000 --> 01:00:03,000
.

984
01:00:03,000 --> 01:00:06,000
.

985
01:00:06,000 --> 01:00:09,000
.

986
01:00:09,000 --> 01:00:12,000
.

987
01:00:12,000 --> 01:00:15,000
.

988
01:00:15,000 --> 01:00:18,000
.

989
01:00:18,000 --> 01:00:21,000
.

990
01:00:21,000 --> 01:00:24,000
.

991
01:00:24,000 --> 01:00:27,000
.

992
01:00:27,000 --> 01:00:30,000
.

993
01:00:30,000 --> 01:00:33,000
.

994
01:00:33,000 --> 01:00:36,000
.

995
01:00:36,000 --> 01:00:39,000
.

996
01:00:39,000 --> 01:00:42,000
.

997
01:00:42,000 --> 01:00:45,000
.

998
01:00:45,000 --> 01:00:48,000
.

999
01:00:48,000 --> 01:00:51,000
.

1000
01:00:51,000 --> 01:00:54,000
.

1001
01:00:54,000 --> 01:00:57,000
.

1002
01:00:57,000 --> 01:01:00,000
.

1003
01:01:00,000 --> 01:01:03,000
.

1004
01:01:03,000 --> 01:01:06,000
.

1005
01:01:06,000 --> 01:01:09,000
.

1006
01:01:09,000 --> 01:01:12,000
.

1007
01:01:12,000 --> 01:01:15,000
.

1008
01:01:15,000 --> 01:01:18,000
.

1009
01:01:18,000 --> 01:01:21,000
.

1010
01:01:21,000 --> 01:01:24,000
.

1011
01:01:24,000 --> 01:01:27,000
.

1012
01:01:27,000 --> 01:01:30,000
.

1013
01:01:30,000 --> 01:01:33,000
.

1014
01:01:33,000 --> 01:01:36,000
.

1015
01:01:36,000 --> 01:01:39,000
.

1016
01:01:39,000 --> 01:01:42,000
.

1017
01:01:42,000 --> 01:01:45,000
.

1018
01:01:45,000 --> 01:01:48,000
.

1019
01:01:48,000 --> 01:01:51,000
.

1020
01:01:51,000 --> 01:01:54,000
.

1021
01:01:54,000 --> 01:01:57,000
.

1022
01:01:57,000 --> 01:02:00,000
.

1023
01:02:00,000 --> 01:02:03,000
.

1024
01:02:03,000 --> 01:02:06,000
.

1025
01:02:06,000 --> 01:02:09,000
.

1026
01:02:09,000 --> 01:02:12,000
.

1027
01:02:12,000 --> 01:02:15,000
.

1028
01:02:15,000 --> 01:02:18,000
.

1029
01:02:18,000 --> 01:02:21,000
.

1030
01:02:21,000 --> 01:02:24,000
.

1031
01:02:24,000 --> 01:02:27,000
.

1032
01:02:27,000 --> 01:02:30,000
.

1033
01:02:30,000 --> 01:02:33,000
.

1034
01:02:33,000 --> 01:02:36,000
.

1035
01:02:36,000 --> 01:02:39,000
.

1036
01:02:39,000 --> 01:02:42,000
.

1037
01:02:42,000 --> 01:02:45,000
.

1038
01:02:45,000 --> 01:02:48,000
.

1039
01:02:48,000 --> 01:02:51,000
.

1040
01:02:51,000 --> 01:02:54,000
.

1041
01:02:54,000 --> 01:02:57,000
.

1042
01:02:57,000 --> 01:03:00,000
.

1043
01:03:00,000 --> 01:03:03,000
.

1044
01:03:03,000 --> 01:03:06,000
.

1045
01:03:06,000 --> 01:03:09,000
.

1046
01:03:09,000 --> 01:03:12,000
.

1047
01:03:12,000 --> 01:03:15,000
.

1048
01:03:15,000 --> 01:03:18,000
.

1049
01:03:18,000 --> 01:03:21,000
.

1050
01:03:21,000 --> 01:03:24,000
.

1051
01:03:24,000 --> 01:03:27,000
.

1052
01:03:27,000 --> 01:03:30,000
.

1053
01:03:30,000 --> 01:03:33,000
.

1054
01:03:33,000 --> 01:03:36,000
.

1055
01:03:36,000 --> 01:03:39,000
.

1056
01:03:39,000 --> 01:03:42,000
.

1057
01:03:42,000 --> 01:03:45,000
.

1058
01:03:45,000 --> 01:03:48,000
.

1059
01:03:48,000 --> 01:03:51,000
.

1060
01:03:51,000 --> 01:03:54,000
.

1061
01:03:54,000 --> 01:03:57,000
.

1062
01:03:57,000 --> 01:04:00,000
.

1063
01:04:00,000 --> 01:04:03,000
.

1064
01:04:03,000 --> 01:04:06,000
.

1065
01:04:06,000 --> 01:04:09,000
.

1066
01:04:09,000 --> 01:04:12,000
.

1067
01:04:12,000 --> 01:04:15,000
.

1068
01:04:15,000 --> 01:04:18,000
.

1069
01:04:18,000 --> 01:04:21,000
.

1070
01:04:21,000 --> 01:04:24,000
.

1071
01:04:24,000 --> 01:04:27,000
.

1072
01:04:27,000 --> 01:04:30,000
.

1073
01:04:30,000 --> 01:04:33,000
.

1074
01:04:33,000 --> 01:04:36,000
.

1075
01:04:36,000 --> 01:04:39,000
.

1076
01:04:39,000 --> 01:04:42,000
.

1077
01:04:42,000 --> 01:04:45,000
.

1078
01:04:45,000 --> 01:04:48,000
.

1079
01:04:48,000 --> 01:04:51,000
.

1080
01:04:51,000 --> 01:04:54,000
.

1081
01:04:54,000 --> 01:04:57,000
.

1082
01:04:57,000 --> 01:05:00,000
.

1083
01:05:00,000 --> 01:05:03,000
.

1084
01:05:03,000 --> 01:05:06,000
.

1085
01:05:06,000 --> 01:05:09,000
.

1086
01:05:09,000 --> 01:05:12,000
.

1087
01:05:12,000 --> 01:05:15,000
.

1088
01:05:15,000 --> 01:05:18,000
.

1089
01:05:18,000 --> 01:05:21,000
.

1090
01:05:21,000 --> 01:05:24,000
.

1091
01:05:24,000 --> 01:05:27,000
.

1092
01:05:27,000 --> 01:05:30,000
.

1093
01:05:30,000 --> 01:05:33,000
.

1094
01:05:33,000 --> 01:05:36,000
.

1095
01:05:36,000 --> 01:05:39,000
.

1096
01:05:39,000 --> 01:05:42,000
.

1097
01:05:42,000 --> 01:05:45,000
.

1098
01:05:45,000 --> 01:05:48,000
.

1099
01:05:48,000 --> 01:05:51,000
.

1100
01:05:51,000 --> 01:05:54,000
.

1101
01:05:54,000 --> 01:05:57,000
.

1102
01:05:57,000 --> 01:06:00,000
.

1103
01:06:00,000 --> 01:06:03,000
.

1104
01:06:03,000 --> 01:06:06,000
.

1105
01:06:06,000 --> 01:06:09,000
.

1106
01:06:09,000 --> 01:06:12,000
.

1107
01:06:12,000 --> 01:06:15,000
.

1108
01:06:15,000 --> 01:06:18,000
.

1109
01:06:18,000 --> 01:06:21,000
.

1110
01:06:21,000 --> 01:06:24,000
.

1111
01:06:24,000 --> 01:06:27,000
.

1112
01:06:27,000 --> 01:06:30,000
.

1113
01:06:30,000 --> 01:06:33,000
.

1114
01:06:33,000 --> 01:06:36,000
.

1115
01:06:36,000 --> 01:06:39,000
.

1116
01:06:39,000 --> 01:06:42,000
.

1117
01:06:42,000 --> 01:06:45,000
.

1118
01:06:45,000 --> 01:06:48,000
.

1119
01:06:48,000 --> 01:06:51,000
.

1120
01:06:51,000 --> 01:06:54,000
.

1121
01:06:54,000 --> 01:06:57,000
.

1122
01:06:57,000 --> 01:07:00,000
.

1123
01:07:00,000 --> 01:07:03,000
.

1124
01:07:03,000 --> 01:07:06,000
.

1125
01:07:06,000 --> 01:07:09,000
.

1126
01:07:09,000 --> 01:07:12,000
.

1127
01:07:12,000 --> 01:07:15,000
.

1128
01:07:15,000 --> 01:07:18,000
.

1129
01:07:18,000 --> 01:07:21,000
.

1130
01:07:21,000 --> 01:07:24,000
.

1131
01:07:24,000 --> 01:07:27,000
.

1132
01:07:27,000 --> 01:07:30,000
.

1133
01:07:30,000 --> 01:07:33,000
.

1134
01:07:33,000 --> 01:07:36,000
.

1135
01:07:36,000 --> 01:07:39,000
.

1136
01:07:39,000 --> 01:07:42,000
.

1137
01:07:42,000 --> 01:07:45,000
.

1138
01:07:45,000 --> 01:07:48,000
.

1139
01:07:48,000 --> 01:07:51,000
.

1140
01:07:51,000 --> 01:07:54,000
.

1141
01:07:54,000 --> 01:07:57,000
.

1142
01:07:57,000 --> 01:08:00,000
.

1143
01:08:00,000 --> 01:08:03,000
.

1144
01:08:03,000 --> 01:08:06,000
.

1145
01:08:06,000 --> 01:08:09,000
.

1146
01:08:09,000 --> 01:08:12,000
.

1147
01:08:12,000 --> 01:08:15,000
.

1148
01:08:15,000 --> 01:08:18,000
.

1149
01:08:18,000 --> 01:08:21,000
.

1150
01:08:21,000 --> 01:08:24,000
.

1151
01:08:24,000 --> 01:08:27,000
.

1152
01:08:27,000 --> 01:08:30,000
.

1153
01:08:30,000 --> 01:08:33,000
.

1154
01:08:33,000 --> 01:08:36,000
.

1155
01:08:36,000 --> 01:08:39,000
.

1156
01:08:39,000 --> 01:08:42,000
.

1157
01:08:42,000 --> 01:08:45,000
.

1158
01:08:45,000 --> 01:08:48,000
.

1159
01:08:48,000 --> 01:08:51,000
.

1160
01:08:51,000 --> 01:08:54,000
.

1161
01:08:54,000 --> 01:08:57,000
.

1162
01:08:57,000 --> 01:09:00,000
.

1163
01:09:00,000 --> 01:09:03,000
.

1164
01:09:03,000 --> 01:09:06,000
.

1165
01:09:06,000 --> 01:09:09,000
.

1166
01:09:09,000 --> 01:09:12,000
.

1167
01:09:12,000 --> 01:09:15,000
.

1168
01:09:15,000 --> 01:09:18,000
.

1169
01:09:18,000 --> 01:09:21,000
.

1170
01:09:21,000 --> 01:09:24,000
.

1171
01:09:24,000 --> 01:09:27,000
.

1172
01:09:27,000 --> 01:09:30,000
.

1173
01:09:30,000 --> 01:09:33,000
.

1174
01:09:33,000 --> 01:09:36,000
.

1175
01:09:36,000 --> 01:09:39,000
.

1176
01:09:39,000 --> 01:09:42,000
.

1177
01:09:42,000 --> 01:09:45,000
.

1178
01:09:45,000 --> 01:09:48,000
.

1179
01:09:48,000 --> 01:09:51,000
.

1180
01:09:51,000 --> 01:09:54,000
.

1181
01:09:54,000 --> 01:09:57,000
.

1182
01:09:57,000 --> 01:10:00,000
.

1183
01:10:00,000 --> 01:10:03,000
.

1184
01:10:03,000 --> 01:10:06,000
.

1185
01:10:06,000 --> 01:10:09,000
.

1186
01:10:09,000 --> 01:10:12,000
.

1187
01:10:12,000 --> 01:10:15,000
.

1188
01:10:15,000 --> 01:10:18,000
.

1189
01:10:18,000 --> 01:10:21,000
.

1190
01:10:21,000 --> 01:10:24,000
.

1191
01:10:24,000 --> 01:10:27,000
.

1192
01:10:27,000 --> 01:10:30,000
.

1193
01:10:30,000 --> 01:10:33,000
.

1194
01:10:33,000 --> 01:10:36,000
.

1195
01:10:36,000 --> 01:10:39,000
.

1196
01:10:39,000 --> 01:10:42,000
.

1197
01:10:42,000 --> 01:10:45,000
.

1198
01:10:45,000 --> 01:10:48,000
.

1199
01:10:48,000 --> 01:10:51,000
.

1200
01:10:51,000 --> 01:10:54,000
.

1201
01:10:54,000 --> 01:10:57,000
.

1202
01:10:57,000 --> 01:11:00,000
.

1203
01:11:00,000 --> 01:11:03,000
.

1204
01:11:03,000 --> 01:11:06,000
.

1205
01:11:06,000 --> 01:11:09,000
.

1206
01:11:09,000 --> 01:11:12,000
.

1207
01:11:12,000 --> 01:11:15,000
.

1208
01:11:15,000 --> 01:11:18,000
.

1209
01:11:18,000 --> 01:11:21,000
.

1210
01:11:21,000 --> 01:11:24,000
.

1211
01:11:24,000 --> 01:11:27,000
.

1212
01:11:27,000 --> 01:11:30,000
.

1213
01:11:30,000 --> 01:11:33,000
.

1214
01:11:33,000 --> 01:11:36,000
.

1215
01:11:36,000 --> 01:11:39,000
.

1216
01:11:39,000 --> 01:11:42,000
.

1217
01:11:42,000 --> 01:11:45,000
.

1218
01:11:45,000 --> 01:11:48,000
.

1219
01:11:48,000 --> 01:11:51,000
.

1220
01:11:51,000 --> 01:11:54,000
.

1221
01:11:54,000 --> 01:11:57,000
.

1222
01:11:57,000 --> 01:12:00,000
.

1223
01:12:00,000 --> 01:12:03,000
.

1224
01:12:03,000 --> 01:12:06,000
.

1225
01:12:06,000 --> 01:12:09,000
.

1226
01:12:09,000 --> 01:12:12,000
.

1227
01:12:12,000 --> 01:12:15,000
.

1228
01:12:15,000 --> 01:12:18,000
.

1229
01:12:18,000 --> 01:12:21,000
.

1230
01:12:21,000 --> 01:12:24,000
.

1231
01:12:24,000 --> 01:12:27,000
.

1232
01:12:27,000 --> 01:12:30,000
.

1233
01:12:30,000 --> 01:12:33,000
.

1234
01:12:33,000 --> 01:12:36,000
.

1235
01:12:36,000 --> 01:12:39,000
.

1236
01:12:39,000 --> 01:12:42,000
.

1237
01:12:42,000 --> 01:12:45,000
.

1238
01:12:45,000 --> 01:12:48,000
.

1239
01:12:48,000 --> 01:12:51,000
.

1240
01:12:51,000 --> 01:12:54,000
.

1241
01:12:54,000 --> 01:12:57,000
.

1242
01:12:57,000 --> 01:13:00,000
.

1243
01:13:00,000 --> 01:13:03,000
.

1244
01:13:03,000 --> 01:13:06,000
.

1245
01:13:06,000 --> 01:13:09,000
.

1246
01:13:09,000 --> 01:13:12,000
.

1247
01:13:12,000 --> 01:13:15,000
.

1248
01:13:15,000 --> 01:13:18,000
.

1249
01:13:18,000 --> 01:13:21,000
.

1250
01:13:21,000 --> 01:13:24,000
.

1251
01:13:24,000 --> 01:13:27,000
.

1252
01:13:27,000 --> 01:13:30,000
.

1253
01:13:30,000 --> 01:13:33,000
.

1254
01:13:33,000 --> 01:13:36,000
.

1255
01:13:36,000 --> 01:13:39,000
.

1256
01:13:39,000 --> 01:13:42,000
.

1257
01:13:42,000 --> 01:13:45,000
.

1258
01:13:45,000 --> 01:13:48,000
.

1259
01:13:48,000 --> 01:13:51,000
.

1260
01:13:51,000 --> 01:13:54,000
.

1261
01:13:54,000 --> 01:13:57,000
.

1262
01:13:57,000 --> 01:14:00,000
.

1263
01:14:00,000 --> 01:14:03,000
.

1264
01:14:03,000 --> 01:14:06,000
.

1265
01:14:06,000 --> 01:14:09,000
.

1266
01:14:09,000 --> 01:14:12,000
.

1267
01:14:12,000 --> 01:14:15,000
.

1268
01:14:15,000 --> 01:14:18,000
.

1269
01:14:18,000 --> 01:14:21,000
.

1270
01:14:21,000 --> 01:14:24,000
.

1271
01:14:24,000 --> 01:14:27,000
.

1272
01:14:27,000 --> 01:14:30,000
.

1273
01:14:30,000 --> 01:14:33,000
.

1274
01:14:33,000 --> 01:14:36,000
.

1275
01:14:36,000 --> 01:14:39,000
.

1276
01:14:39,000 --> 01:14:42,000
.

1277
01:14:42,000 --> 01:14:45,000
.

1278
01:14:45,000 --> 01:14:48,000
.

1279
01:14:48,000 --> 01:14:51,000
.

1280
01:14:51,000 --> 01:14:54,000
.

1281
01:14:54,000 --> 01:14:57,000
.

1282
01:14:57,000 --> 01:15:00,000
.

1283
01:15:00,000 --> 01:15:03,000
.

1284
01:15:03,000 --> 01:15:06,000
.

1285
01:15:06,000 --> 01:15:09,000
.

1286
01:15:09,000 --> 01:15:12,000
.

1287
01:15:12,000 --> 01:15:15,000
.

1288
01:15:15,000 --> 01:15:18,000
.

1289
01:15:18,000 --> 01:15:21,000
.

1290
01:15:21,000 --> 01:15:24,000
.

1291
01:15:24,000 --> 01:15:27,000
.

1292
01:15:27,000 --> 01:15:30,000
.

1293
01:15:30,000 --> 01:15:33,000
.

1294
01:15:33,000 --> 01:15:36,000
.

1295
01:15:36,000 --> 01:15:39,000
.

1296
01:15:39,000 --> 01:15:42,000
.

1297
01:15:42,000 --> 01:15:45,000
.

1298
01:15:45,000 --> 01:15:48,000
.

1299
01:15:48,000 --> 01:15:51,000
.

1300
01:15:51,000 --> 01:15:54,000
.

1301
01:15:54,000 --> 01:15:57,000
.

1302
01:15:57,000 --> 01:16:00,000
.

1303
01:16:00,000 --> 01:16:03,000
.

1304
01:16:03,000 --> 01:16:06,000
.

1305
01:16:06,000 --> 01:16:09,000
.

1306
01:16:09,000 --> 01:16:12,000
.

1307
01:16:12,000 --> 01:16:15,000
.

1308
01:16:15,000 --> 01:16:18,000
.

1309
01:16:18,000 --> 01:16:21,000
.

1310
01:16:21,000 --> 01:16:24,000
.

1311
01:16:24,000 --> 01:16:27,000
.

1312
01:16:27,000 --> 01:16:30,000
.

1313
01:16:30,000 --> 01:16:33,000
.

1314
01:16:33,000 --> 01:16:36,000
.

1315
01:16:36,000 --> 01:16:39,000
.

1316
01:16:39,000 --> 01:16:42,000
.

1317
01:16:42,000 --> 01:16:45,000
.

1318
01:16:45,000 --> 01:16:48,000
.

1319
01:16:48,000 --> 01:16:51,000
.

1320
01:16:51,000 --> 01:16:54,000
.

1321
01:16:54,000 --> 01:16:57,000
.

1322
01:16:57,000 --> 01:17:00,000
.

1323
01:17:00,000 --> 01:17:03,000
.

1324
01:17:03,000 --> 01:17:06,000
.

1325
01:17:06,000 --> 01:17:09,000
.

1326
01:17:09,000 --> 01:17:12,000
.

1327
01:17:13,000 --> 01:17:18,000
.

1328
01:17:18,000 --> 01:17:21,000
.

1329
01:17:21,000 --> 01:17:24,000
.

1330
01:17:24,000 --> 01:17:27,000
.

1331
01:17:27,000 --> 01:17:30,000
.

1332
01:17:30,000 --> 01:17:33,000
.

1333
01:17:33,000 --> 01:17:36,000
.

1334
01:17:36,000 --> 01:17:39,000
.

1335
01:17:39,000 --> 01:17:42,000
.

1336
01:17:42,000 --> 01:17:45,000
.

1337
01:17:45,000 --> 01:17:48,000
.

1338
01:17:48,000 --> 01:17:51,000
.

1339
01:17:51,000 --> 01:17:54,000
.

1340
01:17:54,000 --> 01:17:57,000
.

1341
01:17:57,000 --> 01:18:00,000
.

1342
01:18:00,000 --> 01:18:03,000
.

1343
01:18:03,000 --> 01:18:06,000
.

1344
01:18:06,000 --> 01:18:09,000
.

1345
01:18:09,000 --> 01:18:12,000
.

1346
01:18:12,000 --> 01:18:15,000
.

1347
01:18:15,000 --> 01:18:18,000
.

1348
01:18:18,000 --> 01:18:21,000
.

1349
01:18:21,000 --> 01:18:24,000
.

1350
01:18:24,000 --> 01:18:27,000
.

1351
01:18:27,000 --> 01:18:30,000
.

1352
01:18:30,000 --> 01:18:33,000
.

1353
01:18:33,000 --> 01:18:36,000
.

1354
01:18:36,000 --> 01:18:39,000
.

1355
01:18:39,000 --> 01:18:42,000
.

1356
01:18:42,000 --> 01:18:45,000
.

1357
01:18:45,000 --> 01:18:48,000
.

1358
01:18:48,000 --> 01:18:51,000
.

1359
01:18:51,000 --> 01:18:54,000
.

1360
01:18:54,000 --> 01:18:57,000
.

1361
01:18:57,000 --> 01:19:00,000
.

1362
01:19:00,000 --> 01:19:03,000
.

1363
01:19:03,000 --> 01:19:06,000
.

1364
01:19:06,000 --> 01:19:09,000
.

1365
01:19:09,000 --> 01:19:12,000
.

1366
01:19:12,000 --> 01:19:15,000
.

1367
01:19:15,000 --> 01:19:18,000
.

1368
01:19:18,000 --> 01:19:21,000
.

1369
01:19:21,000 --> 01:19:24,000
.

1370
01:19:24,000 --> 01:19:27,000
.

1371
01:19:27,000 --> 01:19:30,000
.

1372
01:19:30,000 --> 01:19:33,000
.

1373
01:19:33,000 --> 01:19:36,000
.

1374
01:19:36,000 --> 01:19:39,000
.

1375
01:19:39,000 --> 01:19:42,000
.

1376
01:19:42,000 --> 01:19:45,000
.

1377
01:19:45,000 --> 01:19:48,000
.

1378
01:19:48,000 --> 01:19:51,000
.

1379
01:19:51,000 --> 01:19:54,000
.

1380
01:19:54,000 --> 01:19:57,000
.

1381
01:19:57,000 --> 01:20:00,000
.

1382
01:20:00,000 --> 01:20:03,000
.

1383
01:20:03,000 --> 01:20:06,000
.

1384
01:20:06,000 --> 01:20:09,000
.

1385
01:20:09,000 --> 01:20:12,000
.

1386
01:20:12,000 --> 01:20:15,000
.

1387
01:20:15,000 --> 01:20:18,000
.

1388
01:20:18,000 --> 01:20:21,000
.

1389
01:20:21,000 --> 01:20:24,000
.

1390
01:20:24,000 --> 01:20:27,000
.

1391
01:20:27,000 --> 01:20:30,000
.

1392
01:20:30,000 --> 01:20:33,000
.

1393
01:20:33,000 --> 01:20:36,000
.

1394
01:20:36,000 --> 01:20:39,000
.

1395
01:20:39,000 --> 01:20:42,000
.

1396
01:20:42,000 --> 01:20:45,000
.

1397
01:20:45,000 --> 01:20:48,000
.

1398
01:20:48,000 --> 01:20:51,000
.

1399
01:20:51,000 --> 01:20:54,000
.

1400
01:20:54,000 --> 01:20:57,000
.

1401
01:20:57,000 --> 01:21:00,000
.

1402
01:21:00,000 --> 01:21:03,000
.

1403
01:21:03,000 --> 01:21:06,000
.

1404
01:21:06,000 --> 01:21:09,000
.

1405
01:21:09,000 --> 01:21:12,000
.

1406
01:21:12,000 --> 01:21:15,000
.

1407
01:21:15,000 --> 01:21:18,000
.

1408
01:21:18,000 --> 01:21:21,000
.

1409
01:21:21,000 --> 01:21:24,000
.

1410
01:21:24,000 --> 01:21:27,000
.

1411
01:21:27,000 --> 01:21:30,000
.

1412
01:21:30,000 --> 01:21:33,000
.

1413
01:21:33,000 --> 01:21:36,000
.

1414
01:21:36,000 --> 01:21:39,000
.

1415
01:21:39,000 --> 01:21:42,000
.

1416
01:21:42,000 --> 01:21:45,000
.

1417
01:21:45,000 --> 01:21:48,000
.

1418
01:21:48,000 --> 01:21:51,000
.

1419
01:21:51,000 --> 01:21:54,000
.

1420
01:21:54,000 --> 01:21:57,000
.

1421
01:21:57,000 --> 01:22:00,000
.

1422
01:22:00,000 --> 01:22:03,000
.

1423
01:22:03,000 --> 01:22:06,000
.

1424
01:22:06,000 --> 01:22:09,000
.

1425
01:22:09,000 --> 01:22:12,000
.

1426
01:22:12,000 --> 01:22:15,000
.

1427
01:22:15,000 --> 01:22:18,000
.

1428
01:22:18,000 --> 01:22:21,000
.

1429
01:22:21,000 --> 01:22:24,000
.

1430
01:22:24,000 --> 01:22:27,000
.

1431
01:22:27,000 --> 01:22:30,000
.

1432
01:22:30,000 --> 01:22:33,000
.

1433
01:22:33,000 --> 01:22:36,000
.

1434
01:22:36,000 --> 01:22:39,000
.

1435
01:22:39,000 --> 01:22:42,000
.

1436
01:22:42,000 --> 01:22:45,000
.

1437
01:22:45,000 --> 01:22:48,000
.

1438
01:22:48,000 --> 01:22:51,000
.

1439
01:22:51,000 --> 01:22:54,000
.

1440
01:22:54,000 --> 01:22:57,000
.

1441
01:22:57,000 --> 01:23:00,000
.

1442
01:23:00,000 --> 01:23:03,000
.

1443
01:23:03,000 --> 01:23:06,000
.

1444
01:23:06,000 --> 01:23:09,000
.

1445
01:23:09,000 --> 01:23:12,000
.

1446
01:23:12,000 --> 01:23:15,000
.

1447
01:23:15,000 --> 01:23:18,000
.

1448
01:23:18,000 --> 01:23:21,000
.

1449
01:23:21,000 --> 01:23:24,000
.

1450
01:23:24,000 --> 01:23:27,000
.

1451
01:23:27,000 --> 01:23:30,000
.

1452
01:23:30,000 --> 01:23:33,000
.

1453
01:23:33,000 --> 01:23:36,000
.

1454
01:23:36,000 --> 01:23:39,000
.

1455
01:23:39,000 --> 01:23:42,000
.

1456
01:23:42,000 --> 01:23:45,000
.

1457
01:23:45,000 --> 01:23:48,000
.

1458
01:23:48,000 --> 01:23:51,000
.

1459
01:23:51,000 --> 01:23:54,000
.

1460
01:23:54,000 --> 01:23:57,000
.

1461
01:23:57,000 --> 01:24:00,000
.

1462
01:24:00,000 --> 01:24:03,000
.

1463
01:24:03,000 --> 01:24:06,000
.

1464
01:24:06,000 --> 01:24:09,000
.

1465
01:24:09,000 --> 01:24:12,000
.

1466
01:24:12,000 --> 01:24:15,000
.

1467
01:24:15,000 --> 01:24:18,000
.

1468
01:24:18,000 --> 01:24:21,000
.

1469
01:24:21,000 --> 01:24:24,000
.

1470
01:24:24,000 --> 01:24:27,000
.

1471
01:24:27,000 --> 01:24:30,000
.

1472
01:24:30,000 --> 01:24:33,000
.

1473
01:24:33,000 --> 01:24:36,000
.

1474
01:24:36,000 --> 01:24:39,000
.

1475
01:24:39,000 --> 01:24:42,000
.

1476
01:24:42,000 --> 01:24:45,000
.

1477
01:24:45,000 --> 01:24:48,000
.

1478
01:24:48,000 --> 01:24:51,000
.

1479
01:24:51,000 --> 01:24:54,000
.

1480
01:24:54,000 --> 01:24:57,000
.

1481
01:24:57,000 --> 01:25:00,000
.

1482
01:25:00,000 --> 01:25:03,000
.

1483
01:25:03,000 --> 01:25:06,000
.

1484
01:25:06,000 --> 01:25:09,000
.

1485
01:25:09,000 --> 01:25:12,000
.

1486
01:25:12,000 --> 01:25:15,000
.

1487
01:25:15,000 --> 01:25:18,000
.

1488
01:25:18,000 --> 01:25:21,000
.

1489
01:25:21,000 --> 01:25:24,000
.

1490
01:25:24,000 --> 01:25:27,000
.

1491
01:25:27,000 --> 01:25:30,000
.

1492
01:25:30,000 --> 01:25:33,000
.

1493
01:25:33,000 --> 01:25:36,000
.

1494
01:25:36,000 --> 01:25:39,000
.

1495
01:25:39,000 --> 01:25:42,000
.

1496
01:25:42,000 --> 01:25:45,000
.

1497
01:25:45,000 --> 01:25:48,000
.

1498
01:25:48,000 --> 01:25:51,000
.

1499
01:25:51,000 --> 01:25:54,000
.

1500
01:25:54,000 --> 01:25:57,000
.

1501
01:25:57,000 --> 01:26:00,000
.

1502
01:26:00,000 --> 01:26:03,000
.

1503
01:26:03,000 --> 01:26:06,000
.

1504
01:26:06,000 --> 01:26:09,000
.

1505
01:26:09,000 --> 01:26:12,000
.

1506
01:26:12,000 --> 01:26:15,000
.

1507
01:26:15,000 --> 01:26:18,000
.

1508
01:26:18,000 --> 01:26:21,000
.

1509
01:26:21,000 --> 01:26:24,000
.

1510
01:26:24,000 --> 01:26:27,000
.

1511
01:26:27,000 --> 01:26:30,000
.

1512
01:26:30,000 --> 01:26:33,000
.

1513
01:26:33,000 --> 01:26:36,000
.

1514
01:26:36,000 --> 01:26:39,000
.

1515
01:26:39,000 --> 01:26:42,000
.

1516
01:26:42,000 --> 01:26:45,000
.

1517
01:26:45,000 --> 01:26:48,000
.

1518
01:26:48,000 --> 01:26:51,000
.

1519
01:26:51,000 --> 01:26:54,000
.

1520
01:26:54,000 --> 01:26:57,000
.

1521
01:26:57,000 --> 01:27:00,000
.

1522
01:27:00,000 --> 01:27:03,000
.

1523
01:27:03,000 --> 01:27:06,000
.

1524
01:27:06,000 --> 01:27:09,000
.

1525
01:27:09,000 --> 01:27:12,000
.

1526
01:27:12,000 --> 01:27:15,000
.

1527
01:27:15,000 --> 01:27:18,000
.

1528
01:27:18,000 --> 01:27:21,000
.

1529
01:27:21,000 --> 01:27:24,000
.

1530
01:27:24,000 --> 01:27:27,000
.

1531
01:27:27,000 --> 01:27:30,000
.

1532
01:27:30,000 --> 01:27:33,000
.

1533
01:27:33,000 --> 01:27:36,000
.

1534
01:27:36,000 --> 01:27:39,000
.

1535
01:27:39,000 --> 01:27:42,000
.

1536
01:27:42,000 --> 01:27:45,000
.

1537
01:27:45,000 --> 01:27:48,000
.

1538
01:27:48,000 --> 01:27:51,000
.

1539
01:27:51,000 --> 01:27:54,000
.

1540
01:27:54,000 --> 01:27:57,000
.

1541
01:27:57,000 --> 01:28:00,000
.

1542
01:28:00,000 --> 01:28:03,000
.

1543
01:28:03,000 --> 01:28:06,000
.

1544
01:28:06,000 --> 01:28:09,000
.

1545
01:28:09,000 --> 01:28:12,000
.

1546
01:28:12,000 --> 01:28:15,000
.

1547
01:28:15,000 --> 01:28:18,000
.

1548
01:28:18,000 --> 01:28:21,000
.

1549
01:28:21,000 --> 01:28:24,000
.

1550
01:28:24,000 --> 01:28:27,000
.

1551
01:28:27,000 --> 01:28:30,000
.

1552
01:28:30,000 --> 01:28:33,000
.

1553
01:28:33,000 --> 01:28:36,000
.

1554
01:28:36,000 --> 01:28:39,000
.

1555
01:28:39,000 --> 01:28:42,000
.

1556
01:28:42,000 --> 01:28:45,000
.

1557
01:28:45,000 --> 01:28:48,000
.

1558
01:28:48,000 --> 01:28:51,000
.

1559
01:28:51,000 --> 01:28:54,000
.

1560
01:28:54,000 --> 01:28:57,000
.

1561
01:28:57,000 --> 01:29:00,000
.

1562
01:29:00,000 --> 01:29:03,000
.

1563
01:29:03,000 --> 01:29:06,000
.

1564
01:29:06,000 --> 01:29:09,000
.

1565
01:29:09,000 --> 01:29:12,000
.

1566
01:29:12,000 --> 01:29:15,000
.

1567
01:29:15,000 --> 01:29:18,000
.

1568
01:29:18,000 --> 01:29:21,000
.

1569
01:29:21,000 --> 01:29:24,000
.

1570
01:29:24,000 --> 01:29:27,000
.

1571
01:29:27,000 --> 01:29:30,000
.

1572
01:29:30,000 --> 01:29:33,000
.

1573
01:29:33,000 --> 01:29:36,000
.

1574
01:29:36,000 --> 01:29:39,000
.

1575
01:29:39,000 --> 01:29:42,000
.

1576
01:29:42,000 --> 01:29:45,000
.

1577
01:29:45,000 --> 01:29:48,000
.

1578
01:29:48,000 --> 01:29:51,000
.

1579
01:29:51,000 --> 01:29:54,000
.

1580
01:29:54,000 --> 01:29:57,000
.

1581
01:29:57,000 --> 01:30:00,000
.

1582
01:30:00,000 --> 01:30:03,000
.

1583
01:30:03,000 --> 01:30:06,000
.

1584
01:30:06,000 --> 01:30:09,000
.

1585
01:30:09,000 --> 01:30:12,000
.

1586
01:30:12,000 --> 01:30:15,000
.

1587
01:30:15,000 --> 01:30:18,000
.

1588
01:30:18,000 --> 01:30:21,000
.

1589
01:30:21,000 --> 01:30:24,000
.

1590
01:30:24,000 --> 01:30:27,000
.

1591
01:30:27,000 --> 01:30:30,000
.

1592
01:30:30,000 --> 01:30:33,000
.

1593
01:30:33,000 --> 01:30:36,000
.

1594
01:30:36,000 --> 01:30:39,000
.

1595
01:30:39,000 --> 01:30:42,000
.

1596
01:30:42,000 --> 01:30:45,000
.

1597
01:30:45,000 --> 01:30:48,000
.

1598
01:30:48,000 --> 01:30:51,000
.

1599
01:30:51,000 --> 01:30:54,000
.

1600
01:30:54,000 --> 01:30:57,000
.

1601
01:30:57,000 --> 01:31:00,000
.

1602
01:31:00,000 --> 01:31:03,000
.

1603
01:31:03,000 --> 01:31:06,000
.

1604
01:31:06,000 --> 01:31:09,000
.

1605
01:31:09,000 --> 01:31:12,000
.

1606
01:31:12,000 --> 01:31:15,000
.

1607
01:31:15,000 --> 01:31:18,000
.

1608
01:31:18,000 --> 01:31:21,000
.

1609
01:31:21,000 --> 01:31:24,000
.

1610
01:31:24,000 --> 01:31:27,000
.

1611
01:31:27,000 --> 01:31:30,000
.

1612
01:31:30,000 --> 01:31:33,000
.

1613
01:31:33,000 --> 01:31:36,000
.

1614
01:31:36,000 --> 01:31:39,000
.

1615
01:31:39,000 --> 01:31:42,000
.

1616
01:31:42,000 --> 01:31:45,000
.

1617
01:31:45,000 --> 01:31:48,000
.

1618
01:31:48,000 --> 01:31:51,000
.

1619
01:31:51,000 --> 01:31:54,000
.

1620
01:31:54,000 --> 01:31:57,000
.

1621
01:31:57,000 --> 01:32:00,000
.

1622
01:32:00,000 --> 01:32:03,000
.

1623
01:32:03,000 --> 01:32:06,000
.

1624
01:32:06,000 --> 01:32:09,000
.

1625
01:32:09,000 --> 01:32:12,000
.

1626
01:32:12,000 --> 01:32:15,000
.

1627
01:32:15,000 --> 01:32:18,000
.

1628
01:32:18,000 --> 01:32:21,000
.

1629
01:32:21,000 --> 01:32:24,000
.

1630
01:32:24,000 --> 01:32:27,000
.

1631
01:32:27,000 --> 01:32:30,000
.

1632
01:32:30,000 --> 01:32:33,000
.

1633
01:32:33,000 --> 01:32:36,000
.

1634
01:32:36,000 --> 01:32:39,000
.

1635
01:32:39,000 --> 01:32:42,000
.

1636
01:32:42,000 --> 01:32:45,000
.

1637
01:32:45,000 --> 01:32:48,000
.

1638
01:32:48,000 --> 01:32:51,000
.

1639
01:32:51,000 --> 01:32:54,000
.

1640
01:32:54,000 --> 01:32:57,000
.

1641
01:32:57,000 --> 01:33:00,000
.

1642
01:33:00,000 --> 01:33:03,000
.

1643
01:33:03,000 --> 01:33:06,000
.

1644
01:33:06,000 --> 01:33:09,000
.

1645
01:33:09,000 --> 01:33:12,000
.

1646
01:33:12,000 --> 01:33:15,000
.

1647
01:33:15,000 --> 01:33:18,000
.

1648
01:33:18,000 --> 01:33:21,000
.

1649
01:33:21,000 --> 01:33:24,000
.

1650
01:33:24,000 --> 01:33:27,000
.

1651
01:33:27,000 --> 01:33:30,000
.

1652
01:33:30,000 --> 01:33:33,000
.

1653
01:33:33,000 --> 01:33:36,000
.

1654
01:33:36,000 --> 01:33:39,000
.

1655
01:33:39,000 --> 01:33:42,000
.

1656
01:33:42,000 --> 01:33:45,000
.

1657
01:33:45,000 --> 01:33:48,000
.

1658
01:33:48,000 --> 01:33:51,000
.

1659
01:33:51,000 --> 01:33:54,000
.

1660
01:33:54,000 --> 01:33:57,000
.

1661
01:33:57,000 --> 01:34:00,000
.

1662
01:34:00,000 --> 01:34:03,000
.

1663
01:34:03,000 --> 01:34:06,000
.

1664
01:34:06,000 --> 01:34:09,000
.

1665
01:34:09,000 --> 01:34:12,000
.

1666
01:34:12,000 --> 01:34:15,000
.

1667
01:34:15,000 --> 01:34:18,000
.

1668
01:34:18,000 --> 01:34:21,000
.

1669
01:34:21,000 --> 01:34:24,000
.

1670
01:34:24,000 --> 01:34:27,000
.

1671
01:34:27,000 --> 01:34:30,000
.

1672
01:34:30,000 --> 01:34:33,000
.

1673
01:34:33,000 --> 01:34:36,000
.

1674
01:34:36,000 --> 01:34:39,000
.

1675
01:34:39,000 --> 01:34:42,000
.

1676
01:34:42,000 --> 01:34:45,000
.

1677
01:34:45,000 --> 01:34:48,000
.

1678
01:34:48,000 --> 01:34:51,000
.

1679
01:34:51,000 --> 01:34:54,000
.

1680
01:34:54,000 --> 01:34:57,000
.

1681
01:34:57,000 --> 01:35:00,000
.

1682
01:35:00,000 --> 01:35:03,000
.

1683
01:35:03,000 --> 01:35:06,000
.

1684
01:35:06,000 --> 01:35:09,000
.

1685
01:35:09,000 --> 01:35:12,000
.

1686
01:35:12,000 --> 01:35:15,000
.

1687
01:35:15,000 --> 01:35:18,000
.

1688
01:35:18,000 --> 01:35:21,000
.

1689
01:35:21,000 --> 01:35:24,000
.

1690
01:35:24,000 --> 01:35:27,000
.

1691
01:35:27,000 --> 01:35:30,000
.

1692
01:35:30,000 --> 01:35:33,000
.

1693
01:35:33,000 --> 01:35:36,000
.

1694
01:35:36,000 --> 01:35:39,000
.

1695
01:35:39,000 --> 01:35:42,000
.

1696
01:35:42,000 --> 01:35:45,000
.

1697
01:35:45,000 --> 01:35:48,000
.

1698
01:35:48,000 --> 01:35:51,000
.

1699
01:35:51,000 --> 01:35:54,000
.

1700
01:35:54,000 --> 01:35:57,000
.

1701
01:35:57,000 --> 01:36:00,000
.

1702
01:36:00,000 --> 01:36:03,000
.

1703
01:36:03,000 --> 01:36:06,000
.

1704
01:36:06,000 --> 01:36:09,000
.

1705
01:36:09,000 --> 01:36:12,000
.

1706
01:36:12,000 --> 01:36:15,000
.

1707
01:36:15,000 --> 01:36:18,000
.

1708
01:36:18,000 --> 01:36:21,000
.

1709
01:36:21,000 --> 01:36:24,000
.

1710
01:36:24,000 --> 01:36:27,000
.

1711
01:36:27,000 --> 01:36:30,000
.

1712
01:36:30,000 --> 01:36:33,000
.

1713
01:36:33,000 --> 01:36:36,000
.

1714
01:36:36,000 --> 01:36:39,000
.

1715
01:36:39,000 --> 01:36:42,000
.

1716
01:36:42,000 --> 01:36:45,000
.

1717
01:36:45,000 --> 01:36:48,000
.

1718
01:36:48,000 --> 01:36:51,000
.

1719
01:36:51,000 --> 01:36:54,000
.

1720
01:36:54,000 --> 01:36:57,000
.

1721
01:36:57,000 --> 01:37:00,000
.

1722
01:37:00,000 --> 01:37:03,000
.

1723
01:37:03,000 --> 01:37:06,000
.

1724
01:37:06,000 --> 01:37:09,000
.

1725
01:37:09,000 --> 01:37:12,000
.

1726
01:37:12,000 --> 01:37:15,000
.

1727
01:37:15,000 --> 01:37:18,000
.

1728
01:37:18,000 --> 01:37:21,000
.

1729
01:37:21,000 --> 01:37:24,000
.

1730
01:37:24,000 --> 01:37:27,000
.

1731
01:37:27,000 --> 01:37:30,000
.

1732
01:37:30,000 --> 01:37:33,000
.

1733
01:37:33,000 --> 01:37:36,000
.

1734
01:37:36,000 --> 01:37:39,000
.

1735
01:37:39,000 --> 01:37:42,000
.

1736
01:37:42,000 --> 01:37:45,000
.

1737
01:37:45,000 --> 01:37:48,000
.

1738
01:37:48,000 --> 01:37:51,000
.

1739
01:37:51,000 --> 01:37:54,000
.

1740
01:37:54,000 --> 01:37:57,000
.

1741
01:37:57,000 --> 01:38:00,000
.

1742
01:38:00,000 --> 01:38:03,000
.

1743
01:38:03,000 --> 01:38:06,000
.

1744
01:38:06,000 --> 01:38:09,000
.

1745
01:38:09,000 --> 01:38:12,000
.

1746
01:38:12,000 --> 01:38:15,000
.

1747
01:38:15,000 --> 01:38:18,000
.

1748
01:38:18,000 --> 01:38:21,000
.

1749
01:38:21,000 --> 01:38:24,000
.

1750
01:38:24,000 --> 01:38:27,000
.

1751
01:38:27,000 --> 01:38:30,000
.

1752
01:38:30,000 --> 01:38:33,000
.

1753
01:38:33,000 --> 01:38:36,000
.

1754
01:38:36,000 --> 01:38:39,000
.

1755
01:38:39,000 --> 01:38:42,000
.

1756
01:38:42,000 --> 01:38:45,000
.

1757
01:38:45,000 --> 01:38:48,000
.

1758
01:38:48,000 --> 01:38:51,000
.

1759
01:38:51,000 --> 01:38:54,000
.

1760
01:38:54,000 --> 01:38:57,000
.

1761
01:38:57,000 --> 01:39:00,000
.

1762
01:39:00,000 --> 01:39:03,000
.

1763
01:39:03,000 --> 01:39:06,000
.

1764
01:39:06,000 --> 01:39:09,000
.

1765
01:39:09,000 --> 01:39:12,000
.

1766
01:39:12,000 --> 01:39:15,000
.

1767
01:39:15,000 --> 01:39:18,000
.

1768
01:39:18,000 --> 01:39:21,000
.

1769
01:39:21,000 --> 01:39:24,000
.

1770
01:39:24,000 --> 01:39:27,000
.

1771
01:39:27,000 --> 01:39:30,000
.

1772
01:39:30,000 --> 01:39:33,000
.

1773
01:39:33,000 --> 01:39:36,000
.

1774
01:39:36,000 --> 01:39:39,000
.

1775
01:39:39,000 --> 01:39:42,000
.

1776
01:39:42,000 --> 01:39:45,000
.

1777
01:39:45,000 --> 01:39:48,000
.

1778
01:39:48,000 --> 01:39:51,000
.

1779
01:39:51,000 --> 01:39:54,000
.

1780
01:39:54,000 --> 01:39:57,000
.

1781
01:39:57,000 --> 01:40:00,000
.

1782
01:40:00,000 --> 01:40:03,000
.

1783
01:40:03,000 --> 01:40:06,000
.

1784
01:40:06,000 --> 01:40:09,000
.

1785
01:40:09,000 --> 01:40:12,000
.

1786
01:40:12,000 --> 01:40:15,000
.

1787
01:40:15,000 --> 01:40:18,000
.

1788
01:40:18,000 --> 01:40:21,000
.

1789
01:40:21,000 --> 01:40:24,000
.

1790
01:40:24,000 --> 01:40:27,000
.

1791
01:40:27,000 --> 01:40:30,000
.

1792
01:40:30,000 --> 01:40:33,000
.

1793
01:40:33,000 --> 01:40:36,000
.

1794
01:40:36,000 --> 01:40:39,000
.

1795
01:40:39,000 --> 01:40:42,000
.

1796
01:40:42,000 --> 01:40:45,000
.

1797
01:40:45,000 --> 01:40:48,000
.

1798
01:40:48,000 --> 01:40:51,000
.

1799
01:40:51,000 --> 01:40:54,000
.

1800
01:40:54,000 --> 01:40:57,000
.

1801
01:40:57,000 --> 01:41:00,000
.

1802
01:41:00,000 --> 01:41:03,000
.

1803
01:41:03,000 --> 01:41:06,000
.

1804
01:41:06,000 --> 01:41:09,000
.

1805
01:41:09,000 --> 01:41:12,000
.

1806
01:41:12,000 --> 01:41:15,000
.

1807
01:41:15,000 --> 01:41:18,000
.

1808
01:41:18,000 --> 01:41:21,000
.

1809
01:41:21,000 --> 01:41:24,000
.

1810
01:41:24,000 --> 01:41:27,000
.

1811
01:41:27,000 --> 01:41:30,000
.

1812
01:41:30,000 --> 01:41:33,000
.

1813
01:41:33,000 --> 01:41:36,000
.

1814
01:41:36,000 --> 01:41:39,000
.

1815
01:41:39,000 --> 01:41:42,000
.

1816
01:41:42,000 --> 01:41:45,000
.

1817
01:41:45,000 --> 01:41:48,000
.

1818
01:41:48,000 --> 01:41:51,000
.

1819
01:41:51,000 --> 01:41:54,000
.

1820
01:41:54,000 --> 01:41:57,000
.

1821
01:41:57,000 --> 01:42:00,000
.

1822
01:42:28,000 --> 01:42:31,000
.

1823
01:42:31,000 --> 01:42:34,000
.

1824
01:42:34,000 --> 01:42:37,000
.

1825
01:42:37,000 --> 01:42:40,000
.

1826
01:42:40,000 --> 01:42:43,000
.

1827
01:42:43,000 --> 01:42:46,000
.

1828
01:42:46,000 --> 01:42:49,000
.

1829
01:42:49,000 --> 01:42:52,000
.

1830
01:42:52,000 --> 01:42:55,000
.

1831
01:42:55,000 --> 01:42:58,000
.

1832
01:42:58,000 --> 01:43:01,000
.

1833
01:43:01,000 --> 01:43:04,000
.

1834
01:43:04,000 --> 01:43:07,000
.

1835
01:43:07,000 --> 01:43:10,000
.

1836
01:43:10,000 --> 01:43:13,000
.

1837
01:43:13,000 --> 01:43:16,000
.

1838
01:43:16,000 --> 01:43:19,000
.

1839
01:43:19,000 --> 01:43:22,000
.

1840
01:43:22,000 --> 01:43:25,000
.

1841
01:43:25,000 --> 01:43:28,000
.

1842
01:43:28,000 --> 01:43:31,000
.

1843
01:43:31,000 --> 01:43:34,000
.

1844
01:43:34,000 --> 01:43:37,000
.

1845
01:43:37,000 --> 01:43:40,000
.

1846
01:43:40,000 --> 01:43:43,000
.

1847
01:43:43,000 --> 01:43:46,000
.

1848
01:43:46,000 --> 01:43:49,000
.

1849
01:43:49,000 --> 01:43:52,000
.

1850
01:43:52,000 --> 01:43:55,000
.

1851
01:43:55,000 --> 01:43:58,000
.

1852
01:43:58,000 --> 01:44:01,000
.

1853
01:44:01,000 --> 01:44:04,000
.

1854
01:44:04,000 --> 01:44:07,000
.

1855
01:44:07,000 --> 01:44:10,000
.

1856
01:44:10,000 --> 01:44:13,000
.

1857
01:44:13,000 --> 01:44:16,000
.

1858
01:44:16,000 --> 01:44:19,000
.

1859
01:44:19,000 --> 01:44:22,000
.

1860
01:44:22,000 --> 01:44:25,000
.

1861
01:44:25,000 --> 01:44:28,000
.

1862
01:44:28,000 --> 01:44:31,000
.

1863
01:44:31,000 --> 01:44:34,000
.

1864
01:44:34,000 --> 01:44:37,000
.

1865
01:44:37,000 --> 01:44:40,000
.

1866
01:44:40,000 --> 01:44:43,000
.

1867
01:44:43,000 --> 01:44:46,000
.

1868
01:44:46,000 --> 01:44:49,000
.

1869
01:44:49,000 --> 01:44:52,000
.

1870
01:44:52,000 --> 01:44:55,000
.

1871
01:44:55,000 --> 01:44:58,000
.

1872
01:44:58,000 --> 01:45:01,000
.

1873
01:45:01,000 --> 01:45:04,000
.

1874
01:45:04,000 --> 01:45:07,000
.

1875
01:45:07,000 --> 01:45:10,000
.

1876
01:45:10,000 --> 01:45:13,000
.

1877
01:45:13,000 --> 01:45:16,000
.

1878
01:45:16,000 --> 01:45:19,000
.

1879
01:45:19,000 --> 01:45:22,000
.

1880
01:45:22,000 --> 01:45:25,000
.

1881
01:45:25,000 --> 01:45:28,000
.

1882
01:45:28,000 --> 01:45:31,000
.

1883
01:45:31,000 --> 01:45:34,000
.

1884
01:45:34,000 --> 01:45:37,000
.

1885
01:45:37,000 --> 01:45:40,000
.

1886
01:45:40,000 --> 01:45:43,000
.

1887
01:45:43,000 --> 01:45:46,000
.

1888
01:45:46,000 --> 01:45:49,000
.

1889
01:45:49,000 --> 01:45:52,000
.

1890
01:45:52,000 --> 01:45:55,000
.

1891
01:45:55,000 --> 01:45:58,000
.

1892
01:45:58,000 --> 01:46:01,000
.

1893
01:46:01,000 --> 01:46:04,000
.

1894
01:46:04,000 --> 01:46:07,000
.

1895
01:46:07,000 --> 01:46:10,000
.

1896
01:46:10,000 --> 01:46:13,000
.

1897
01:46:13,000 --> 01:46:16,000
.

1898
01:46:16,000 --> 01:46:19,000
.

1899
01:46:19,000 --> 01:46:22,000
.

1900
01:46:22,000 --> 01:46:25,000
.

1901
01:46:25,000 --> 01:46:28,000
.

1902
01:46:28,000 --> 01:46:31,000
.

1903
01:46:31,000 --> 01:46:34,000
.

1904
01:46:34,000 --> 01:46:37,000
.

1905
01:46:37,000 --> 01:46:40,000
.

1906
01:46:40,000 --> 01:46:43,000
.

1907
01:46:43,000 --> 01:46:46,000
.

1908
01:46:46,000 --> 01:46:49,000
.

1909
01:46:49,000 --> 01:46:52,000
.

1910
01:46:52,000 --> 01:46:55,000
.

1911
01:46:55,000 --> 01:46:58,000
.

1912
01:46:58,000 --> 01:47:01,000
.

1913
01:47:01,000 --> 01:47:04,000
.

1914
01:47:04,000 --> 01:47:07,000
.

1915
01:47:07,000 --> 01:47:10,000
.

1916
01:47:10,000 --> 01:47:13,000
.

1917
01:47:13,000 --> 01:47:16,000
.

1918
01:47:16,000 --> 01:47:19,000
.

1919
01:47:19,000 --> 01:47:22,000
.

1920
01:47:22,000 --> 01:47:25,000
.

1921
01:47:25,000 --> 01:47:28,000
.

1922
01:47:28,000 --> 01:47:31,000
.

1923
01:47:31,000 --> 01:47:34,000
.

1924
01:47:34,000 --> 01:47:37,000
.

1925
01:47:37,000 --> 01:47:40,000
.

1926
01:47:40,000 --> 01:47:43,000
.

1927
01:47:43,000 --> 01:47:46,000
.

1928
01:47:46,000 --> 01:47:49,000
.

1929
01:47:49,000 --> 01:47:52,000
.

1930
01:47:52,000 --> 01:47:55,000
.

1931
01:47:55,000 --> 01:47:58,000
.

1932
01:47:58,000 --> 01:48:01,000
.

1933
01:48:01,000 --> 01:48:04,000
.

1934
01:48:04,000 --> 01:48:07,000
.

1935
01:48:07,000 --> 01:48:10,000
.

1936
01:48:10,000 --> 01:48:13,000
.

1937
01:48:13,000 --> 01:48:16,000
.

1938
01:48:16,000 --> 01:48:19,000
.

1939
01:48:19,000 --> 01:48:22,000
.

1940
01:48:22,000 --> 01:48:25,000
.

1941
01:48:25,000 --> 01:48:28,000
.

1942
01:48:28,000 --> 01:48:31,000
.

1943
01:48:31,000 --> 01:48:34,000
.

1944
01:48:34,000 --> 01:48:37,000
.

1945
01:48:37,000 --> 01:48:40,000
.

1946
01:48:40,000 --> 01:48:43,000
.

1947
01:48:43,000 --> 01:48:46,000
.

1948
01:48:46,000 --> 01:48:49,000
.

1949
01:48:49,000 --> 01:48:52,000
.

1950
01:48:52,000 --> 01:48:55,000
.

1951
01:48:55,000 --> 01:48:58,000
.

1952
01:48:58,000 --> 01:49:01,000
.

1953
01:49:01,000 --> 01:49:04,000
.

1954
01:49:04,000 --> 01:49:07,000
.

1955
01:49:07,000 --> 01:49:10,000
.

1956
01:49:10,000 --> 01:49:13,000
.

1957
01:49:13,000 --> 01:49:16,000
.

1958
01:49:16,000 --> 01:49:19,000
.

1959
01:49:19,000 --> 01:49:22,000
.

1960
01:49:22,000 --> 01:49:25,000
.

1961
01:49:25,000 --> 01:49:28,000
.

1962
01:49:28,000 --> 01:49:31,000
.

1963
01:49:31,000 --> 01:49:34,000
.

1964
01:49:34,000 --> 01:49:37,000
.

1965
01:49:37,000 --> 01:49:40,000
.

1966
01:49:40,000 --> 01:49:43,000
.

1967
01:49:43,000 --> 01:49:46,000
.

1968
01:49:46,000 --> 01:49:49,000
.

1969
01:49:49,000 --> 01:49:52,000
.

1970
01:49:52,000 --> 01:49:55,000
.

1971
01:49:55,000 --> 01:49:58,000
.

1972
01:49:58,000 --> 01:50:01,000
.

1973
01:50:01,000 --> 01:50:04,000
.

1974
01:50:04,000 --> 01:50:07,000
.

1975
01:50:07,000 --> 01:50:10,000
.

1976
01:50:10,000 --> 01:50:13,000
.

1977
01:50:13,000 --> 01:50:16,000
.

1978
01:50:16,000 --> 01:50:19,000
.

1979
01:50:19,000 --> 01:50:22,000
.

1980
01:50:22,000 --> 01:50:25,000
.

1981
01:50:25,000 --> 01:50:28,000
.

1982
01:50:28,000 --> 01:50:31,000
.

1983
01:50:31,000 --> 01:50:34,000
.

1984
01:50:34,000 --> 01:50:37,000
.

1985
01:50:37,000 --> 01:50:40,000
.

1986
01:50:40,000 --> 01:50:43,000
.

1987
01:50:43,000 --> 01:50:46,000
.

1988
01:50:46,000 --> 01:50:49,000
.

1989
01:50:49,000 --> 01:50:52,000
.

1990
01:50:52,000 --> 01:50:55,000
.

1991
01:50:55,000 --> 01:50:58,000
.

1992
01:50:58,000 --> 01:51:01,000
.

1993
01:51:01,000 --> 01:51:04,000
.

1994
01:51:04,000 --> 01:51:07,000
.

1995
01:51:07,000 --> 01:51:10,000
.

1996
01:51:10,000 --> 01:51:13,000
.

1997
01:51:13,000 --> 01:51:16,000
.

1998
01:51:16,000 --> 01:51:19,000
.

1999
01:51:19,000 --> 01:51:22,000
.

2000
01:51:22,000 --> 01:51:25,000
.

2001
01:51:25,000 --> 01:51:28,000
.

2002
01:51:28,000 --> 01:51:31,000
.

2003
01:51:31,000 --> 01:51:34,000
.

2004
01:51:34,000 --> 01:51:37,000
.

2005
01:51:37,000 --> 01:51:40,000
.

2006
01:51:40,000 --> 01:51:43,000
.

2007
01:51:43,000 --> 01:51:46,000
.

2008
01:51:46,000 --> 01:51:49,000
.

2009
01:51:49,000 --> 01:51:52,000
.

2010
01:51:52,000 --> 01:51:55,000
.

2011
01:51:55,000 --> 01:51:58,000
.

2012
01:51:58,000 --> 01:52:01,000
.

2013
01:52:01,000 --> 01:52:04,000
.

2014
01:52:04,000 --> 01:52:07,000
.

2015
01:52:07,000 --> 01:52:10,000
.

2016
01:52:10,000 --> 01:52:13,000
.

2017
01:52:13,000 --> 01:52:16,000
.

2018
01:52:16,000 --> 01:52:19,000
.

2019
01:52:19,000 --> 01:52:22,000
.

2020
01:52:22,000 --> 01:52:25,000
.

2021
01:52:25,000 --> 01:52:28,000
.

2022
01:52:28,000 --> 01:52:31,000
.

2023
01:52:31,000 --> 01:52:34,000
.

2024
01:52:34,000 --> 01:52:37,000
.

2025
01:52:37,000 --> 01:52:40,000
.

2026
01:52:40,000 --> 01:52:43,000
.

2027
01:52:43,000 --> 01:52:46,000
.

2028
01:52:46,000 --> 01:52:49,000
.

2029
01:52:49,000 --> 01:52:52,000
.

2030
01:52:52,000 --> 01:52:55,000
.

2031
01:52:55,000 --> 01:52:58,000
.

2032
01:52:58,000 --> 01:53:01,000
.

2033
01:53:01,000 --> 01:53:04,000
.

2034
01:53:04,000 --> 01:53:07,000
.

2035
01:53:07,000 --> 01:53:10,000
.

2036
01:53:10,000 --> 01:53:13,000
.

2037
01:53:13,000 --> 01:53:16,000
.

2038
01:53:16,000 --> 01:53:19,000
.

2039
01:53:19,000 --> 01:53:22,000
.

2040
01:53:22,000 --> 01:53:25,000
.

2041
01:53:25,000 --> 01:53:28,000
.

2042
01:53:28,000 --> 01:53:31,000
.

2043
01:53:31,000 --> 01:53:34,000
.

2044
01:53:34,000 --> 01:53:37,000
.

2045
01:53:37,000 --> 01:53:40,000
.

2046
01:53:40,000 --> 01:53:43,000
.

2047
01:53:43,000 --> 01:53:46,000
.

2048
01:53:46,000 --> 01:53:49,000
.

2049
01:53:49,000 --> 01:53:52,000
.

2050
01:53:52,000 --> 01:53:55,000
.

2051
01:53:55,000 --> 01:53:58,000
.

2052
01:53:58,000 --> 01:54:01,000
.

2053
01:54:01,000 --> 01:54:04,000
.

2054
01:54:04,000 --> 01:54:07,000
.

2055
01:54:07,000 --> 01:54:10,000
.

2056
01:54:10,000 --> 01:54:13,000
.

2057
01:54:13,000 --> 01:54:16,000
.

2058
01:54:16,000 --> 01:54:19,000
.

2059
01:54:19,000 --> 01:54:22,000
.

2060
01:54:22,000 --> 01:54:25,000
.

2061
01:54:25,000 --> 01:54:28,000
.

2062
01:54:28,000 --> 01:54:31,000
.

2063
01:54:31,000 --> 01:54:34,000
.

2064
01:54:34,000 --> 01:54:37,000
.

2065
01:54:37,000 --> 01:54:40,000
.

2066
01:54:40,000 --> 01:54:43,000
.

2067
01:54:43,000 --> 01:54:46,000
.

2068
01:54:46,000 --> 01:54:49,000
.

2069
01:54:49,000 --> 01:54:52,000
.

2070
01:54:52,000 --> 01:54:55,000
.

2071
01:54:55,000 --> 01:54:58,000
.

2072
01:54:58,000 --> 01:55:01,000
.

2073
01:55:01,000 --> 01:55:04,000
.

2074
01:55:04,000 --> 01:55:07,000
.

2075
01:55:07,000 --> 01:55:10,000
.

2076
01:55:10,000 --> 01:55:13,000
.

2077
01:55:13,000 --> 01:55:16,000
.

2078
01:55:16,000 --> 01:55:19,000
.

2079
01:55:19,000 --> 01:55:22,000
.

2080
01:55:22,000 --> 01:55:25,000
.

2081
01:55:25,000 --> 01:55:28,000
.

2082
01:55:28,000 --> 01:55:31,000
.

2083
01:55:31,000 --> 01:55:34,000
.

2084
01:55:34,000 --> 01:55:37,000
.

2085
01:55:37,000 --> 01:55:40,000
.

2086
01:55:40,000 --> 01:55:43,000
.

2087
01:55:43,000 --> 01:55:46,000
.

2088
01:55:46,000 --> 01:55:49,000
.

2089
01:55:49,000 --> 01:55:52,000
.

2090
01:55:52,000 --> 01:55:55,000
.

2091
01:55:55,000 --> 01:55:58,000
.

2092
01:55:58,000 --> 01:56:01,000
.

2093
01:56:01,000 --> 01:56:04,000
.

2094
01:56:04,000 --> 01:56:07,000
.

2095
01:56:07,000 --> 01:56:10,000
.

2096
01:56:10,000 --> 01:56:13,000
.

2097
01:56:13,000 --> 01:56:16,000
.

2098
01:56:16,000 --> 01:56:19,000
.

2099
01:56:19,000 --> 01:56:22,000
.

2100
01:56:22,000 --> 01:56:25,000
.

2101
01:56:25,000 --> 01:56:28,000
.

2102
01:56:28,000 --> 01:56:31,000
.

2103
01:56:31,000 --> 01:56:34,000
.

2104
01:56:34,000 --> 01:56:37,000
.

2105
01:56:37,000 --> 01:56:40,000
.

2106
01:56:40,000 --> 01:56:43,000
.

2107
01:56:43,000 --> 01:56:46,000
.

2108
01:56:46,000 --> 01:56:49,000
.

2109
01:56:49,000 --> 01:56:52,000
.

2110
01:56:52,000 --> 01:56:55,000
.

2111
01:56:55,000 --> 01:56:58,000
.

2112
01:56:58,000 --> 01:57:01,000
.

2113
01:57:01,000 --> 01:57:04,000
.

2114
01:57:04,000 --> 01:57:07,000
.

2115
01:57:07,000 --> 01:57:10,000
.

2116
01:57:10,000 --> 01:57:13,000
.

2117
01:57:13,000 --> 01:57:16,000
.

2118
01:57:16,000 --> 01:57:19,000
.

2119
01:57:19,000 --> 01:57:22,000
.

2120
01:57:22,000 --> 01:57:25,000
.

2121
01:57:25,000 --> 01:57:28,000
.

2122
01:57:28,000 --> 01:57:31,000
.

2123
01:57:31,000 --> 01:57:34,000
.

2124
01:57:34,000 --> 01:57:37,000
.

2125
01:57:37,000 --> 01:57:40,000
.

2126
01:57:40,000 --> 01:57:43,000
.

2127
01:57:43,000 --> 01:57:46,000
.

2128
01:57:46,000 --> 01:57:49,000
.

2129
01:57:49,000 --> 01:57:52,000
.

2130
01:57:52,000 --> 01:57:55,000
.

2131
01:57:55,000 --> 01:57:58,000
.

2132
01:57:58,000 --> 01:58:01,000
.

2133
01:58:01,000 --> 01:58:04,000
.

2134
01:58:04,000 --> 01:58:07,000
.

2135
01:58:07,000 --> 01:58:10,000
.

2136
01:58:10,000 --> 01:58:13,000
.

2137
01:58:13,000 --> 01:58:16,000
.

2138
01:58:16,000 --> 01:58:19,000
.

2139
01:58:19,000 --> 01:58:22,000
.

2140
01:58:22,000 --> 01:58:25,000
.

2141
01:58:25,000 --> 01:58:28,000
.

2142
01:58:28,000 --> 01:58:31,000
.

2143
01:58:31,000 --> 01:58:34,000
.

2144
01:58:34,000 --> 01:58:37,000
.

2145
01:58:37,000 --> 01:58:40,000
.

2146
01:58:40,000 --> 01:58:43,000
.

2147
01:58:43,000 --> 01:58:46,000
.

2148
01:58:46,000 --> 01:58:49,000
.

2149
01:58:49,000 --> 01:58:52,000
.

2150
01:58:52,000 --> 01:58:55,000
.

2151
01:58:55,000 --> 01:58:58,000
.

2152
01:58:58,000 --> 01:59:01,000
.

2153
01:59:01,000 --> 01:59:04,000
.

2154
01:59:04,000 --> 01:59:07,000
.

2155
01:59:07,000 --> 01:59:10,000
.

2156
01:59:10,000 --> 01:59:13,000
.

2157
01:59:13,000 --> 01:59:16,000
.

2158
01:59:16,000 --> 01:59:19,000
.

2159
01:59:19,000 --> 01:59:22,000
.

2160
01:59:22,000 --> 01:59:25,000
.

2161
01:59:25,000 --> 01:59:28,000
.

2162
01:59:28,000 --> 01:59:31,000
.

2163
01:59:31,000 --> 01:59:34,000
.

2164
01:59:34,000 --> 01:59:37,000
.

2165
01:59:37,000 --> 01:59:40,000
.

2166
01:59:40,000 --> 01:59:43,000
.

2167
01:59:43,000 --> 01:59:46,000
.

2168
01:59:46,000 --> 01:59:49,000
.

2169
01:59:49,000 --> 01:59:52,000
.

2170
01:59:52,000 --> 01:59:55,000
.

2171
01:59:55,000 --> 01:59:58,000
.

2172
01:59:58,000 --> 02:00:01,000
.

2173
02:00:01,000 --> 02:00:04,000
.

2174
02:00:04,000 --> 02:00:07,000
.

2175
02:00:07,000 --> 02:00:10,000
.

2176
02:00:10,000 --> 02:00:13,000
.

2177
02:00:13,000 --> 02:00:16,000
.

2178
02:00:16,000 --> 02:00:19,000
.

2179
02:00:19,000 --> 02:00:22,000
.

2180
02:00:22,000 --> 02:00:25,000
.

2181
02:00:25,000 --> 02:00:28,000
.

2182
02:00:28,000 --> 02:00:31,000
.

2183
02:00:31,000 --> 02:00:34,000
.

2184
02:00:34,000 --> 02:00:37,000
.

2185
02:00:37,000 --> 02:00:40,000
.

2186
02:00:40,000 --> 02:00:43,000
.

2187
02:00:43,000 --> 02:00:46,000
.

2188
02:00:46,000 --> 02:00:49,000
.

2189
02:00:49,000 --> 02:00:52,000
.

2190
02:00:52,000 --> 02:00:55,000
.

2191
02:00:55,000 --> 02:00:58,000
.

2192
02:00:58,000 --> 02:01:01,000
.

2193
02:01:01,000 --> 02:01:04,000
.

2194
02:01:04,000 --> 02:01:07,000
.

2195
02:01:07,000 --> 02:01:10,000
.

2196
02:01:10,000 --> 02:01:13,000
.

2197
02:01:13,000 --> 02:01:16,000
.

2198
02:01:16,000 --> 02:01:19,000
.

2199
02:01:19,000 --> 02:01:22,000
.

2200
02:01:22,000 --> 02:01:25,000
.

2201
02:01:25,000 --> 02:01:28,000
.

2202
02:01:28,000 --> 02:01:31,000
.

2203
02:01:31,000 --> 02:01:34,000
.

2204
02:01:34,000 --> 02:01:37,000
.

2205
02:01:37,000 --> 02:01:40,000
.

2206
02:01:40,000 --> 02:01:43,000
.

2207
02:01:43,000 --> 02:01:46,000
.

2208
02:01:46,000 --> 02:01:49,000
.

2209
02:01:49,000 --> 02:01:52,000
.

2210
02:01:52,000 --> 02:01:55,000
.

2211
02:01:55,000 --> 02:01:58,000
.

2212
02:01:58,000 --> 02:02:01,000
.

2213
02:02:01,000 --> 02:02:04,000
.

2214
02:02:04,000 --> 02:02:07,000
.

2215
02:02:07,000 --> 02:02:10,000
.

2216
02:02:10,000 --> 02:02:13,000
.

2217
02:02:13,000 --> 02:02:16,000
.

2218
02:02:16,000 --> 02:02:19,000
.

2219
02:02:19,000 --> 02:02:22,000
.

2220
02:02:22,000 --> 02:02:25,000
.

2221
02:02:25,000 --> 02:02:28,000
.

2222
02:02:28,000 --> 02:02:31,000
.

2223
02:02:31,000 --> 02:02:34,000
.

2224
02:02:34,000 --> 02:02:37,000
.

2225
02:02:37,000 --> 02:02:40,000
.

2226
02:02:40,000 --> 02:02:43,000
.

2227
02:02:43,000 --> 02:02:46,000
.

2228
02:02:46,000 --> 02:02:49,000
.

2229
02:02:49,000 --> 02:02:52,000
.

2230
02:02:52,000 --> 02:02:55,000
.

2231
02:02:55,000 --> 02:02:58,000
.

2232
02:02:58,000 --> 02:03:01,000
.

2233
02:03:01,000 --> 02:03:04,000
.

2234
02:03:04,000 --> 02:03:07,000
.

2235
02:03:07,000 --> 02:03:10,000
.

2236
02:03:10,000 --> 02:03:13,000
.

2237
02:03:13,000 --> 02:03:16,000
.

2238
02:03:16,000 --> 02:03:19,000
.

2239
02:03:19,000 --> 02:03:22,000
.

2240
02:03:22,000 --> 02:03:25,000
.

2241
02:03:25,000 --> 02:03:28,000
.

2242
02:03:28,000 --> 02:03:31,000
.

2243
02:03:31,000 --> 02:03:34,000
.

2244
02:03:34,000 --> 02:03:37,000
.

2245
02:03:37,000 --> 02:03:40,000
.

2246
02:03:40,000 --> 02:03:43,000
.

2247
02:03:43,000 --> 02:03:46,000
.

2248
02:03:46,000 --> 02:03:49,000
.

2249
02:03:49,000 --> 02:03:52,000
.

2250
02:03:52,000 --> 02:03:55,000
.

2251
02:03:55,000 --> 02:03:58,000
.

2252
02:03:58,000 --> 02:04:01,000
.

2253
02:04:01,000 --> 02:04:04,000
.

2254
02:04:04,000 --> 02:04:07,000
.

2255
02:04:07,000 --> 02:04:10,000
.

2256
02:04:10,000 --> 02:04:13,000
.

2257
02:04:13,000 --> 02:04:16,000
.

2258
02:04:16,000 --> 02:04:19,000
.

2259
02:04:19,000 --> 02:04:22,000
.

2260
02:04:22,000 --> 02:04:25,000
.

2261
02:04:25,000 --> 02:04:28,000
.

2262
02:04:28,000 --> 02:04:31,000
.

2263
02:04:31,000 --> 02:04:34,000
.

2264
02:04:34,000 --> 02:04:37,000
.

2265
02:04:37,000 --> 02:04:40,000
.

2266
02:04:40,000 --> 02:04:43,000
.

2267
02:04:43,000 --> 02:04:46,000
.

2268
02:04:46,000 --> 02:04:49,000
.

2269
02:04:49,000 --> 02:04:52,000
.

2270
02:04:52,000 --> 02:04:55,000
.

2271
02:04:55,000 --> 02:04:58,000
.

2272
02:04:58,000 --> 02:05:01,000
.

2273
02:05:01,000 --> 02:05:04,000
.

2274
02:05:04,000 --> 02:05:07,000
.

2275
02:05:07,000 --> 02:05:10,000
.

2276
02:05:10,000 --> 02:05:13,000
.

2277
02:05:13,000 --> 02:05:16,000
.

2278
02:05:16,000 --> 02:05:19,000
.

2279
02:05:19,000 --> 02:05:22,000
.

2280
02:05:22,000 --> 02:05:25,000
.

2281
02:05:25,000 --> 02:05:28,000
.

2282
02:05:28,000 --> 02:05:31,000
.

2283
02:05:31,000 --> 02:05:34,000
.

2284
02:05:34,000 --> 02:05:37,000
.

2285
02:05:37,000 --> 02:05:40,000
.

2286
02:05:40,000 --> 02:05:43,000
.

2287
02:05:43,000 --> 02:05:46,000
.

2288
02:05:46,000 --> 02:05:49,000
.

2289
02:05:49,000 --> 02:05:52,000
.

2290
02:05:52,000 --> 02:05:55,000
.

2291
02:05:55,000 --> 02:05:58,000
.

2292
02:05:58,000 --> 02:06:01,000
.

2293
02:06:01,000 --> 02:06:04,000
.

2294
02:06:04,000 --> 02:06:07,000
.

2295
02:06:07,000 --> 02:06:10,000
.

2296
02:06:10,000 --> 02:06:13,000
.

2297
02:06:13,000 --> 02:06:16,000
.

2298
02:06:16,000 --> 02:06:19,000
.

2299
02:06:19,000 --> 02:06:22,000
.

